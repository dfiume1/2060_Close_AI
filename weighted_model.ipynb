{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIZqWCd2L6C"
      },
      "source": [
        "# Boosting With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJqYh6u2Qtz"
      },
      "source": [
        "## Markdown Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppdKF2TvGOLo"
      },
      "source": [
        "# Representation\n",
        "\n",
        "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
        "\n",
        "$$E(H, T) = {x \\to sign(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUF8LgSz3Nna"
      },
      "source": [
        "### Check Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKoyt0u2tiR",
        "outputId": "357f42eb-bd03-4dcb-a75b-cd289e2a5cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.5\n",
            "\n",
            "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from packaging.version import parse as Version\n",
        "from platform import python_version\n",
        "\n",
        "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
        "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "except ImportError:\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % sys.version)\n",
        "\n",
        "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
        "    mod = None\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        if pkg in {'PIL'}:\n",
        "            ver = mod.VERSION\n",
        "        else:\n",
        "            ver = mod.__version__\n",
        "        if Version(ver) == Version(min_ver):\n",
        "            print(OK, \"%s version %s is installed.\"\n",
        "                  % (lib, min_ver))\n",
        "        else:\n",
        "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
        "                  % (lib, min_ver, ver))\n",
        "    except ImportError:\n",
        "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
        "    return mod\n",
        "\n",
        "\n",
        "# first check the python version\n",
        "pyversion = Version(python_version())\n",
        "\n",
        "if pyversion >= Version(\"3.12.5\"):\n",
        "    print(OK, \"Python version is %s\" % pyversion)\n",
        "elif pyversion < Version(\"3.12.5\"):\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % pyversion)\n",
        "else:\n",
        "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
        "\n",
        "\n",
        "print()\n",
        "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\",\n",
        "                'pandas': \"2.2.2\"}\n",
        "\n",
        "# now the dependencies\n",
        "for lib, required_version in list(requirements.items()):\n",
        "    import_version(lib, required_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfCnqsOY3WLp"
      },
      "source": [
        "## Model Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npX8gMU3Y0I"
      },
      "source": [
        "### Weak Learner: One Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dFNBypLJ1nJY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def l2_loss_weight(predictions,Y,weights):\n",
        "    '''\n",
        "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
        "        that are weighted\n",
        "        :param Y: A 1D Numpy array with real values (float64)\n",
        "        :param predictions: A 1D Numpy array of the same size of Y\n",
        "        :param weights: A 1D Numpy array of the same size of Y,\n",
        "        :return: Weighted L2 loss using predictions for Y.\n",
        "    '''\n",
        "\n",
        "    return np.sum(weights * (predictions - Y)**2)\n",
        "\n",
        "class OneLayerNN:\n",
        "    '''\n",
        "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        @attrs:\n",
        "            weights: The weights of the neural network model.\n",
        "            batch_size: The number of examples in each batch\n",
        "            learning_rate: The learning rate to use for SGD\n",
        "            epochs: The number of times to pass through the dataset\n",
        "            v: The resulting predictions computed during the forward pass\n",
        "        '''\n",
        "        # initialize self.weights in train()\n",
        "        self.weights = None\n",
        "        self.learning_rate = 0.001\n",
        "        self.epochs = 25\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # initialize self.v in forward_pass()\n",
        "        self.v = None\n",
        "\n",
        "    def train(self, X, Y, data_weights, print_loss=False):\n",
        "        '''\n",
        "        Trains the OneLayerNN model using SGD.\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :param print_loss: If True, print the loss after each epoch.\n",
        "        :return: None\n",
        "        '''\n",
        "        # Initialize weights\n",
        "        input_size = X.shape[1]\n",
        "        self.weights = np.random.uniform(0,1,(1, input_size))\n",
        "        #print(\"Weights Initial Shape: \", self.weights.shape)\n",
        "\n",
        "        # Train network for certain number of epochs\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # Shuffle the examples (X) and labels (Y)\n",
        "            rand_index = np.arange(X.shape[0])\n",
        "            np.random.shuffle(rand_index)\n",
        "            X_s = X[rand_index]\n",
        "            Y_s = Y[rand_index]\n",
        "            data_weights_s = data_weights[rand_index]\n",
        "\n",
        "             # iterate through the examples in batch size increments\n",
        "            for i in range((int(np.ceil(X_s.shape[0] / self.batch_size)))):\n",
        "                X_batch = X_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "                Y_batch = Y_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "                data_weights_batch = data_weights_s[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "\n",
        "\n",
        "                #Perform the forward and backward pass on the current batch\n",
        "                self.forward_pass(X_batch)\n",
        "                self.backward_pass(X_batch, Y_batch, data_weights_batch)\n",
        "\n",
        "            # Print the loss after every epoch\n",
        "            if print_loss:\n",
        "                print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y, data_weights_batch)))\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        '''\n",
        "        Computes the predictions for a single layer given examples X and\n",
        "        stores them in self.v\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :return: None\n",
        "        '''\n",
        "\n",
        "        self.v = np.dot(self.weights, X.T) # + bias (no bias for now?)\n",
        "        #print(\"v shape: \", self.v.shape)\n",
        "\n",
        "\n",
        "\n",
        "    def backward_pass(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Computes the weights gradient and updates self.weights\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: None\n",
        "        '''\n",
        "        # Compute the gradients for the model's weights using backprop\n",
        "        grads = self.backprop(X,Y, data_weights)\n",
        "\n",
        "        # Update the weights using gradient descent\n",
        "        self.gradient_descent(grads)\n",
        "\n",
        "\n",
        "\n",
        "    def backprop(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the average weights gradient for the given batch\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A 1D Numpy array representing the weights gradient\n",
        "        '''\n",
        "        # Compute the average weights gradient\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "\n",
        "        # The gradient dL/dw = -2 * xi * predictons (sum(yi-h(xi)))\n",
        "        m = X.shape[0]\n",
        "        grad_W = (2/m) * np.dot(X.T, data_weights * (self.v - Y))\n",
        "\n",
        "        #print(\"Shape of grad_W: \", grad_W.T.shape)\n",
        "\n",
        "        return grad_W.T\n",
        "\n",
        "    def gradient_descent(self, grad_W):\n",
        "        '''\n",
        "        Updates the weights using the given gradient\n",
        "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
        "        :return: None\n",
        "        '''\n",
        "        # Update the weights using the given gradient and the learning rate\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "        self.weights = self.weights - (self.learning_rate * grad_W)\n",
        "        #print(self.weights)\n",
        "\n",
        "\n",
        "    def loss(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the total squared error on some dataset (X, Y).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the squared error of the model on the dataset\n",
        "        '''\n",
        "        # Perform the forward pass and compute the l2 loss\n",
        "        self.forward_pass(X)\n",
        "        return l2_loss_weight(self.v, Y, data_weights)\n",
        "\n",
        "    def average_loss(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the mean squared error on some dataset (X, Y).\n",
        "        MSE = Total squared error/# of examples\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the mean squared error of the model on the dataset\n",
        "        '''\n",
        "        return self.loss(X, Y, data_weights) / X.shape[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Returns the predicted values for some dataset (X).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :return: 1D Numpy array containing the predicted values for each example\n",
        "        '''\n",
        "        self.forward_pass(X)\n",
        "        return self.v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3JqPeK3hVC"
      },
      "source": [
        "### Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eDK23Q7F3gZJ"
      },
      "outputs": [],
      "source": [
        "class Boosted_NN:\n",
        "  def __init__(self, n_estimators=50, learning_rate=0.01, random_state=1):\n",
        "    self.n_estimators = n_estimators\n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "    self.estimator_weights = np.zeros(self.n_estimators)\n",
        "    self.data_weights = []\n",
        "\n",
        "    # Initialize the estimators\n",
        "    self.estimators = []\n",
        "    for i in range(self.n_estimators):\n",
        "      self.estimators.append(OneLayerNN())\n",
        "\n",
        "  def train(self, X, y):\n",
        "    '''\n",
        "    Trains/Fits the Boosting Model using AdaBoost.\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "    '''\n",
        "    # Initialize the data and estimator weights\n",
        "    num_inputs = X.shape[0]\n",
        "    self.data_weights = np.ones(num_inputs) / num_inputs\n",
        "\n",
        "    # For each round/weak learner\n",
        "    for i in range(self.n_estimators):\n",
        "\n",
        "      # Use the weak learner\n",
        "      weak_learner = self.estimators[i]\n",
        "\n",
        "      # Fit the weak learner\n",
        "      weak_learner.train(X, y, self.data_weights)\n",
        "\n",
        "      y_pred = weak_learner.predict(X).reshape(num_inputs)\n",
        "\n",
        "      e_t = np.sum(self.data_weights * (y-y_pred)**2) / np.sum(self.data_weights) \n",
        "      e_t = e_t / num_inputs\n",
        "      print(\"weighted error\", e_t)\n",
        "      \n",
        "\n",
        "      w_t = 0.5 * np.log((1 / e_t) - 1)\n",
        "      print(\"w_t\", w_t)\n",
        "\n",
        "      self.estimator_weights[i] = w_t\n",
        "      \n",
        "      self.data_weights *= np.exp(-w_t * (y-y_pred))\n",
        "      self.data_weights = np.clip(self.data_weights, a_min=1e-10, a_max=1e5)\n",
        "      self.data_weights /= np.sum(self.data_weights)\n",
        "      # print(f\"Sum of data weights (should be 1): {np.sum(self.data_weights)}\")  \n",
        "\n",
        "\n",
        "  def loss(self, X, Y):\n",
        "      # Get predictions from all learners, then weight them\n",
        "      predictions = np.array([e.predict(X) for e in self.estimators])\n",
        "      weighted_predictions = np.dot(self.estimator_weights, predictions)\n",
        "\n",
        "      # L2 loss\n",
        "      loss = np.mean((Y - weighted_predictions) ** 2)\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    '''\n",
        "    Returns the predicted values for some dataset (X).\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :return: 1D Numpy array containing the predicted values for each example\n",
        "    '''\n",
        "    y_pred = np.array([e.predict(X) for e in self.estimators])\n",
        "    return np.dot(self.estimator_weights, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjphpjYOI1F"
      },
      "source": [
        "## Accuracy on Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-mPAx8l8ONaB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running models on wine.txt dataset\n",
            "----- 1-Layer NN -----\n",
            "Average Training Loss: 0.5758575528276906\n",
            "Average Testing Loss: 0.5477329678880231\n",
            "----- Boosted Neural Network -----\n",
            "weighted error 0.007657439375162092\n",
            "w_t 2.4321953641226\n",
            "weighted error 0.0003138257042718536\n",
            "w_t 4.03317946657497\n",
            "weighted error 2.59673664767669e-05\n",
            "w_t 5.279321988045208\n",
            "weighted error 0.00013488052630650118\n",
            "w_t 4.455493136029844\n",
            "weighted error 0.00026621733376499314\n",
            "w_t 4.1154656428881635\n",
            "weighted error 0.00048483149831293445\n",
            "w_t 3.8156121022425165\n",
            "weighted error 4.3940709991700166e-05\n",
            "w_t 5.016312695627007\n",
            "weighted error 0.00022685506242663874\n",
            "w_t 4.19548617756003\n",
            "weighted error 0.00019613917992222454\n",
            "w_t 4.268244945337454\n",
            "weighted error 4.6261070622535365e-06\n",
            "w_t 6.141895112356867\n",
            "weighted error 0.00045456418502436705\n",
            "w_t 3.847858382805572\n",
            "weighted error 0.00026287880669950404\n",
            "w_t 4.121777264827826\n",
            "weighted error 6.96818958836093e-05\n",
            "w_t 4.78575016651671\n",
            "weighted error 0.00024747587220568564\n",
            "w_t 4.151974980122912\n",
            "weighted error 0.0002493943635627969\n",
            "w_t 4.148112849747499\n",
            "weighted error 0.0001962947646089659\n",
            "w_t 4.267848406680671\n",
            "weighted error 9.248326102698075e-06\n",
            "w_t 5.7955293681585625\n",
            "weighted error 1.0992579146379322e-05\n",
            "w_t 5.709139571621646\n",
            "weighted error 5.03233542839875e-06\n",
            "w_t 6.099810674688952\n",
            "weighted error 1.2342997781106959e-05\n",
            "w_t 5.651204646948944\n",
            "weighted error 1.1545348758938242e-05\n",
            "w_t 5.684608180771136\n",
            "weighted error 0.00029387176758401053\n",
            "w_t 4.066036567748479\n",
            "weighted error 0.0010467668172853472\n",
            "w_t 3.430500885971434\n",
            "weighted error 0.00024340707535011716\n",
            "w_t 4.160265936156216\n",
            "weighted error 2.499560135615673e-05\n",
            "w_t 5.298392849208161\n",
            "weighted error 0.0004511063727101764\n",
            "w_t 3.851678089226259\n",
            "weighted error 0.00011054821564352218\n",
            "w_t 4.5549741185917325\n",
            "weighted error 5.659096147386035e-05\n",
            "w_t 4.889802342109448\n",
            "weighted error 4.419156991707917e-07\n",
            "w_t 7.316073128500672\n",
            "weighted error 9.082738568322544e-05\n",
            "w_t 4.6532294409394765\n",
            "weighted error 6.83930686928063e-05\n",
            "w_t 4.795085338981692\n",
            "weighted error 0.0006027287167136514\n",
            "w_t 3.7067202207003738\n",
            "weighted error 7.515769897975427e-05\n",
            "w_t 4.747923419161114\n",
            "weighted error 0.0002700270021511955\n",
            "w_t 4.108359266266483\n",
            "weighted error 5.748881130843907e-06\n",
            "w_t 6.033249779565925\n",
            "weighted error 0.00023070156960500483\n",
            "w_t 4.1870774304798095\n",
            "weighted error 6.333245131337058e-07\n",
            "w_t 7.136141126931621\n",
            "weighted error 2.205392361820862e-06\n",
            "w_t 6.512301458815788\n",
            "weighted error 8.264276101468027e-07\n",
            "w_t 7.003076341561269\n",
            "weighted error 0.00017893876276335195\n",
            "w_t 4.314143981674361\n",
            "weighted error 1.447424512664139e-05\n",
            "w_t 5.5715626058304375\n",
            "weighted error 2.508143771367737e-06\n",
            "w_t 6.447982551791475\n",
            "weighted error 4.0728656650076834e-07\n",
            "w_t 7.356874198689497\n",
            "weighted error 0.0003979523665198852\n",
            "w_t 3.914390105191288\n",
            "weighted error 1.4987571578994718e-06\n",
            "w_t 6.705436427972451\n",
            "weighted error 8.017626387697816e-06\n",
            "w_t 5.866930061933149\n",
            "weighted error 3.7212261290973744e-07\n",
            "w_t 7.402021030131088\n",
            "weighted error 0.0007101622222101275\n",
            "w_t 3.624653358767001\n",
            "weighted error 1.8397598034121197e-05\n",
            "w_t 5.451636022863605\n",
            "weighted error 3.0113438991869385e-06\n",
            "w_t 6.356560544680783\n",
            "(3918, 12)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "shapes (50,) and (50,1,3918) not aligned: 50 (dim 0) != 1 (dim 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Training Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mloss(X_train_b, Y_train))\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Testing Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mloss(X_test_b, Y_test))\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtest_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwine.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 47\u001b[0m, in \u001b[0;36mtest_models\u001b[0;34m(dataset, test_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(X_train_b, Y_train)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train_b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Training Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Testing Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mloss(X_test_b, Y_test))\n",
            "Cell \u001b[0;32mIn[12], line 54\u001b[0m, in \u001b[0;36mBoosted_NN.loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Get predictions from all learners, then weight them\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators])\n\u001b[0;32m---> 54\u001b[0m     weighted_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# L2 loss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((Y \u001b[38;5;241m-\u001b[39m weighted_predictions) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (50,) and (50,1,3918) not aligned: 50 (dim 0) != 1 (dim 1)"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "\n",
        "\n",
        "def test_models(dataset, test_size=0.2):\n",
        "    '''\n",
        "        Tests OneLayerNN, Boost on a given dataset.\n",
        "        :param dataset The path to the dataset\n",
        "        :return None\n",
        "    '''\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(dataset):\n",
        "        print('The file {} does not exist'.format(dataset))\n",
        "        exit()\n",
        "\n",
        "    # Load in the dataset\n",
        "    data = np.loadtxt(dataset, skiprows = 1)\n",
        "    X, Y = data[:, 1:], data[:, 0]\n",
        "\n",
        "    # Normalize the features\n",
        "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
        "    Y = Y \n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "    print('Running models on {} dataset'.format(dataset))\n",
        "\n",
        "    # Add a bias\n",
        "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
        "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
        "\n",
        "    #### 1-Layer NN ######\n",
        "    print('----- 1-Layer NN -----')\n",
        "    nnmodel = OneLayerNN()\n",
        "    nnmodel.train(X_train_b, Y_train, np.ones(X_train_b.shape[0]), print_loss=False)\n",
        "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0])))\n",
        "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0])))\n",
        "\n",
        "    #### Boosted Neural Networks ######\n",
        "    print('----- Boosted Neural Network -----')\n",
        "    model = Boosted_NN()\n",
        "\n",
        "    model.train(X_train_b, Y_train)\n",
        "\n",
        "    print(X_train_b.shape)\n",
        "    \n",
        "    print('Average Training Loss:', model.loss(X_train_b, Y_train))\n",
        "    print('Average Testing Loss:', model.loss(X_test_b, Y_test))\n",
        "\n",
        "test_models('wine.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h3Us2FgOMgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y7jqYzqTDwBc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data2060",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

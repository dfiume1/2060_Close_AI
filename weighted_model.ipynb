{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvIZqWCd2L6C"
   },
   "source": [
    "# Boosting With Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVJqYh6u2Qtz"
   },
   "source": [
    "## Markdown Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Boosting Algorithm\n",
    "\n",
    "Boosting is an ensemble learning method. The idea of boosting algorithm is to create a single strong learner by combining the predictions of weak learners. This method was proposed by Freund and Schapire in the paper \"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\" in 1995 and mainly applied to classification tasks. In boosting, each weak learner will adjust its weight according to the results of the previous round of weak learners in continuous iterations. Boosting iteratively builds weak classifiers or regressors and this new learner will pay special attention to the samples that were misclassified or having bigger errors in the previous round, so as to better handle these samples that did not perform well in the subsequent iterations. Thus, models can gradually optimize the performance. In the final prediction stage, the prediction results of these weak learners will be weighted and summed according to certain weights to obtain the final prediction result. Based on boosting, Freund and Schapire also proposed AdaBoosting, the adaptive boosting algorithm, in the above paper. At the same time, Drucker further applied it to regression problems in 1997.\n",
    "Adaboosting improves the adaptive mechanism: Adaboosting dynamically adjusts the sample weights and the weights of weak learners according to the error during the iteration process. This adaptive mechanism enables AdaBoosting to gradually optimize the model performance and improve the prediction accuracy.\n",
    "\n",
    "### Advantages\n",
    "- According to Drucker's 1997 paper \"Improving Regressors using Boosting Techniques\", AdaBoosting can achieve higher prediction performance by iteratively training multiple weak learners and reasonably combining their prediction results according to the weights.\n",
    "- The AdaBoosting algorithm is relatively simple and easy to understand and implement.\n",
    "\n",
    "### Disadvantages\n",
    "- Since AdaBoosting gives higher weights to wrongly predicted samples, noise and outliers may cause the model to perform poorly. .\n",
    "- AdaBoosting needs to train multiple weak learners during the training process, the amount of training calculations may be heavy.\n",
    "- If the weak learner is too complex or the number of iterations is too many, AdaBoosting may have overfitting problems.\n",
    "\n",
    "### Modification\n",
    "In this project, we used Adaboosting to implement regression instead of classification problems. The main changes made are as follows:\n",
    "- We chose a 1 layer NN as a weak learner because NN has strong fitting ability and is suitable as a weak learner for regression tasks.\n",
    "- We used mean squared error as the loss function instead of 0/1 loss.\n",
    "\n",
    "\n",
    "### Representation\n",
    "\n",
    "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
    "\n",
    "$$E(H, T) = {x \\to \\frac{1}{T}(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
    "\n",
    "$h_t(x)$: Prediction result from the t-th weak learner.\n",
    "\n",
    "$w_t$ Weight of the t-th learner, determined based on its performance.\n",
    "\n",
    "T: Total number of weak learners in the ensemble.\n",
    "\n",
    "As can be seen from the above, by combining $w_t$ and $h_t(x)$, the weak learner with higher accuracy has a greater weight in the final prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss function quantifies the error between the model's predicted value and the true value, providing guidance for the optimization process.\n",
    "\n",
    "We are using weighted L2 loss for the one-layer neural network and MSE for the boosted ensemble of hypotheses.\n",
    "For each single-layer neural network, loss is defined as:\n",
    "$$L_S(h_{\\bf t}) = \\sum\\limits_{i=1}^m w_m *(y_{i}-h_{\\bf t}({\\bf x}_{i}))^{2}$$\n",
    "\n",
    "where *y*<sub>i</sub> is the target value of *i*<sup>th</sup>\n",
    "sample and $h_{\\bf t}({\\bf x})$ is the predicted value of that\n",
    "sample given the learned model weights, and $w_m$ is the weight for the mth data point.\n",
    "\n",
    "For the ensemble, loss is then defined as:\n",
    "$$L_S(E(H, T)) = \\frac{1}{m}\\sum\\limits_{i=1}^m(y_{i}-E(H, T)({\\bf x}_{i}))^{2}$$\n",
    "\n",
    "### Optimzer\n",
    "\n",
    "The iterative optimization process of AdaBoosting mainly consists of five steps:\n",
    "\n",
    "- First, initialization is performed to assign equal weights to all training samples, where $m$ is the number of samples, $D$ is the weight distribution of each training sample, and $D^{(1)} = \\frac{1}{m}$.\n",
    "\n",
    "- Next, in each iteration, a weak learner $h_t$ is trained using the weighted dataset.\n",
    "\n",
    "- The third step is to calculate the error rate $\\epsilon_t$ of the weak learner.\n",
    "\n",
    "- The fourth step is to calculate the weight $w_t$ of the weak learner and adjust the sample weights $D_i^{(t+1)}$ to pay more attention to the misclassified samples.\n",
    "\n",
    "- Finally, we combine the predictions of all weak learners into the final integrated model.\n",
    "\n",
    "Through the flexible weight adjustment and weighted integration strategy of the prediction results during the iteration process, we can gradually improve the accuracy of the results.\n",
    "\n",
    "below is the pesudoCode:\n",
    "\n",
    "Input:\n",
    "- Training set $S = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_m, y_m)\\}$\n",
    "- Weak learner $Wl$\n",
    "- Number of estimators $T$\n",
    "\n",
    "Initialize:\n",
    "$D^{(1)} = \\left(\\frac{1}{m}, \\ldots, \\frac{1}{m}\\right)$\n",
    "\n",
    "For $t = 1, \\ldots, T$:\n",
    "\n",
    "$h_t = WL(D^{(t)}, S)$\n",
    "\n",
    "$\\epsilon_t = \\sum_{i=1}^m D_i^{(t)} \\mathbb{1}[y_i \\neq h_t(\\mathbf{x}_i)]$\n",
    "\n",
    "$w_t = \\frac{1}{2} \\log \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$\n",
    "\n",
    "$D_i^{(t+1)} = \\frac{D_i^{(t)} \\exp(-w_t y_i h_t(\\mathbf{x}_i))}{\\sum_{j=1}^m D_j^{(t)} \\exp(-w_t y_j h_t(\\mathbf{x}_j))} \\quad \\forall i = 1, \\ldots, m$\n",
    "\n",
    "Output:\n",
    "- The hypothesis: $h_S(\\mathbf{x}) = \\text{sign}\\left(\\sum_{t=1}^T w_t h_t(\\mathbf{x})\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUF8LgSz3Nna"
   },
   "source": [
    "### Check Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHKoyt0u2tiR",
    "outputId": "357f42eb-bd03-4dcb-a75b-cd289e2a5cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.5\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))\n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.5\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.5\"):\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "\n",
    "print()\n",
    "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\",\n",
    "                'pandas': \"2.2.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfCnqsOY3WLp"
   },
   "source": [
    "## Model Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5npX8gMU3Y0I"
   },
   "source": [
    "### Weak Learner: One Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dFNBypLJ1nJY"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def l2_loss_weight(predictions,Y,weights):\n",
    "    '''\n",
    "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
    "        that are weighted\n",
    "        :param Y: A 1D Numpy array with real values (float64)\n",
    "        :param predictions: A 1D Numpy array of the same size of Y\n",
    "        :param weights: A 1D Numpy array of the same size of Y,\n",
    "        :return: Weighted L2 loss using predictions for Y.\n",
    "    '''\n",
    "\n",
    "    return np.sum(weights * (predictions - Y)**2)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class OneLayerNN(BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
    "    '''\n",
    "    def __init__(self, learning_rate = 0.001, num_epochs = 25, batch_size = 1):\n",
    "        '''\n",
    "        @attrs:\n",
    "            weights: The weights of the neural network model.\n",
    "            batch_size: The number of examples in each batch\n",
    "            learning_rate: The learning rate to use for SGD\n",
    "            epochs: The number of times to pass through the dataset\n",
    "            v: The resulting predictions computed during the forward pass\n",
    "        '''\n",
    "        # initialize self.weights in fit()\n",
    "        self.weights = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # initialize self.v in forward_pass()\n",
    "        self.v = None\n",
    "        self.data_weights = None\n",
    "    \n",
    "    def fit(self, X, Y, data_weights=None):\n",
    "        '''\n",
    "        Trains the OneLayerNN model using SGD.\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :param print_loss: If True, print the loss after each epoch.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: initialize weights\n",
    "        num_examples, num_features = X.shape\n",
    "        self.weights = np.random.uniform(0, 1, (1, num_features))\n",
    "        if data_weights is None:\n",
    "            self.data_weights = np.ones(num_examples)\n",
    "        else:\n",
    "            self.data_weights = data_weights\n",
    "        # TODO: Train network for certain number of epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # TODO: Shuffle the examples (X) and labels (Y)\n",
    "            indices = np.random.permutation(num_examples)\n",
    "            X_shuffled = X[indices]\n",
    "            Y_shuffled = Y[indices]\n",
    "            data_weights_shuffled = self.data_weights[indices]\n",
    "        # TODO: We need to iterate over each data point for each epoch\n",
    "        # iterate through the examples in batch size increments\n",
    "            for i in range(num_examples):\n",
    "\n",
    "                x_i = X_shuffled[i].reshape(1, num_features)\n",
    "                y_i = Y_shuffled[i].reshape(1, 1)            \n",
    "                data_i = data_weights_shuffled[i]\n",
    "                # TODO: Perform the forward and backward pass on the current batch\n",
    "                self.forward_pass(x_i)\n",
    "                self.backward_pass(x_i, y_i, data_i)\n",
    "\n",
    "            # Print the loss after every epoch\n",
    "            # if print_loss:\n",
    "            #     print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        '''\n",
    "        Computes the predictions for a single layer given examples X and\n",
    "        stores them in self.v\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO:\n",
    "        self.v = np.dot(self.weights, X.T).flatten()\n",
    "\n",
    "    def backward_pass(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Computes the weights gradient and updates self.weights\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Compute the gradients for the model's weights using backprop\n",
    "        gradient = self.backprop(X, Y)\n",
    "        # TODO: Update the weights using gradient descent\n",
    "        self.gradient_descent(gradient, data_weights)\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''\n",
    "        Returns the average weights gradient for the given batch\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A 1D Numpy array representing the weights gradient\n",
    "        '''\n",
    "        # TODO: Compute the average weights gradient\n",
    "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
    "        loss = self.v - Y\n",
    "        return np.dot(2*loss, X)\n",
    "\n",
    "    def gradient_descent(self, grad_W, data_weights):\n",
    "        '''\n",
    "        Updates the weights using the given gradient\n",
    "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
    "        :return: None\n",
    "        '''\n",
    "        self.weights -= (self.learning_rate * grad_W * data_weights)\n",
    "\n",
    "    def loss(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        # Perform the forward pass and compute the l2 loss\n",
    "        self.forward_pass(X)\n",
    "        return l2_loss_weight(self.v, Y, data_weights)\n",
    "\n",
    "    def average_loss(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y, data_weights) / X.shape[0]\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "            Returns the predicted values for some dataset (X).\n",
    "            :param X: 2D Numpy array where each row contains an example\n",
    "            :return: 1D Numpy array containing the predicted values for each example\n",
    "        '''\n",
    "        self.forward_pass(X)\n",
    "        return self.v\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def l2_loss_weight(predictions,Y,weights):\n",
    "    '''\n",
    "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
    "        that are weighted\n",
    "        :param Y: A 1D Numpy array with real values (float64)\n",
    "        :param predictions: A 1D Numpy array of the same size of Y\n",
    "        :param weights: A 1D Numpy array of the same size of Y,\n",
    "        :return: Weighted L2 loss using predictions for Y.\n",
    "    '''\n",
    "\n",
    "    return np.sum(weights * (predictions - Y)**2)\n",
    "\n",
    "#from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# class OneLayerNN(BaseEstimator, RegressorMixin):\n",
    "#     '''\n",
    "#         One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
    "#     '''\n",
    "#     def __init__(self, learning_rate = 0.001, num_epochs = 25, batch_size = 1):\n",
    "#         '''\n",
    "#         @attrs:\n",
    "#             weights: The weights of the neural network model.\n",
    "#             batch_size: The number of examples in each batch\n",
    "#             learning_rate: The learning rate to use for SGD\n",
    "#             epochs: The number of times to pass through the dataset\n",
    "#             v: The resulting predictions computed during the forward pass\n",
    "#         '''\n",
    "#         # initialize self.weights in fit()\n",
    "#         self.weights = None\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = num_epochs\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#         # initialize self.v in forward_pass()\n",
    "#         self.v = None\n",
    "#         self.data_weights = None\n",
    "    \n",
    "#     def fit(self, X, Y, data_weights=None):\n",
    "#         '''\n",
    "#         Trains the OneLayerNN model using SGD.\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :param print_loss: If True, print the loss after each epoch.\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         # TODO: initialize weights\n",
    "#         num_examples, num_features = X.shape\n",
    "#         self.weights = np.random.uniform(0, 1, (1, num_features))\n",
    "#         if data_weights is None:\n",
    "#             data_weights = np.ones(num_examples)\n",
    "#         else:\n",
    "#             self.data_weights = data_weights\n",
    "#         # TODO: Train network for certain number of epochs\n",
    "#         for epoch in range(self.epochs):\n",
    "#             # TODO: Shuffle the examples (X) and labels (Y)\n",
    "#             indices = np.random.permutation(num_examples)\n",
    "#             X_shuffled = X[indices]\n",
    "#             Y_shuffled = Y[indices]\n",
    "#         # TODO: We need to iterate over each data point for each epoch\n",
    "#         # iterate through the examples in batch size increments\n",
    "#             for i in range(num_examples):\n",
    "\n",
    "#                 x_i = X_shuffled[i].reshape(1, num_features)\n",
    "#                 y_i = Y_shuffled[i].reshape(1, 1)            \n",
    "\n",
    "#                 # TODO: Perform the forward and backward pass on the current batch\n",
    "#                 self.forward_pass(x_i)\n",
    "#                 self.backward_pass(x_i, y_i)\n",
    "\n",
    "#             # Print the loss after every epoch\n",
    "#             # if print_loss:\n",
    "#             #     print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "#     def forward_pass(self, X):\n",
    "#         '''\n",
    "#         Computes the predictions for a single layer given examples X and\n",
    "#         stores them in self.v\n",
    "#         :param X: 2D Numpy array where each row contains an example.\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         # TODO:\n",
    "#         self.v = np.dot(self.weights, X.T).flatten()\n",
    "\n",
    "#     def backward_pass(self, X, Y):\n",
    "#         '''\n",
    "#         Computes the weights gradient and updates self.weights\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         # TODO: Compute the gradients for the model's weights using backprop\n",
    "#         gradient = self.backprop(X, Y)\n",
    "#         # TODO: Update the weights using gradient descent\n",
    "#         self.gradient_descent(gradient)\n",
    "\n",
    "#     def backprop(self, X, Y):\n",
    "#         '''\n",
    "#         Returns the average weights gradient for the given batch\n",
    "#         :param X: 2D Numpy array where each row contains an example.\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: A 1D Numpy array representing the weights gradient\n",
    "#         '''\n",
    "#         # TODO: Compute the average weights gradient\n",
    "#         # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
    "#         loss = self.v - Y\n",
    "#         return np.dot(2*loss, X)\n",
    "\n",
    "#     def gradient_descent(self, grad_W):\n",
    "#         '''\n",
    "#         Updates the weights using the given gradient\n",
    "#         :param grad_W: A 1D Numpy array representing the weights gradient\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         self.weights -= (self.learning_rate * grad_W)\n",
    "\n",
    "#     def loss(self, X, Y, data_weights):\n",
    "#         '''\n",
    "#         Returns the total squared error on some dataset (X, Y).\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: A float which is the squared error of the model on the dataset\n",
    "#         '''\n",
    "#         # Perform the forward pass and compute the l2 loss\n",
    "#         self.forward_pass(X)\n",
    "#         return l2_loss_weight(self.v, Y, data_weights)\n",
    "\n",
    "#     def average_loss(self, X, Y, data_weights):\n",
    "#         '''\n",
    "#         Returns the mean squared error on some dataset (X, Y).\n",
    "#         MSE = Total squared error/# of examples\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: A float which is the mean squared error of the model on the dataset\n",
    "#         '''\n",
    "#         return self.loss(X, Y, data_weights) / X.shape[0]\n",
    "#     def predict(self, X):\n",
    "#         '''\n",
    "#             Returns the predicted values for some dataset (X).\n",
    "#             :param X: 2D Numpy array where each row contains an example\n",
    "#             :return: 1D Numpy array containing the predicted values for each example\n",
    "#         '''\n",
    "#         self.forward_pass(X)\n",
    "#         return self.v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW3JqPeK3hVC"
   },
   "source": [
    "### Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eDK23Q7F3gZJ"
   },
   "outputs": [],
   "source": [
    "class Boosted_Model:\n",
    "  def __init__(self, model, n_estimators=50, learning_rate=0.5, random_state=1):\n",
    "    self.n_estimators = n_estimators\n",
    "    self.learning_rate = learning_rate\n",
    "    self.random_state = random_state\n",
    "    self.estimator_weights = np.zeros(self.n_estimators)\n",
    "    self.data_weights = []\n",
    "\n",
    "    # Initialize the estimators\n",
    "    self.estimators = []\n",
    "    for i in range(self.n_estimators):\n",
    "      if model == \"nn\":\n",
    "        self.estimators.append(OneLayerNN(learning_rate=0.2))\n",
    "      else:\n",
    "        self.estimators.append(DecisionTreeRegressor(max_depth=2))\n",
    "      \n",
    "\n",
    "  def train(self, X, y):\n",
    "    '''\n",
    "    Trains/Fits the Boosting Model using AdaBoost.\n",
    "    :param X: 2D Numpy array where each row contains an example\n",
    "    :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "    '''\n",
    "    # Initialize the data and estimator weights\n",
    "    num_inputs = X.shape[0]\n",
    "    self.data_weights = np.ones(num_inputs) / num_inputs\n",
    "\n",
    "    # For each round/weak learner\n",
    "    for i in range(self.n_estimators):\n",
    "\n",
    "      # Use the weak learner\n",
    "      weak_learner = self.estimators[i]\n",
    "\n",
    "      # Fit the weak learner\n",
    "      weak_learner.fit(X, y, self.data_weights)\n",
    "      # print(self.data_weights)\n",
    "      #print(\"Before Reshape\", weak_learner.predict(X).shape)\n",
    "      y_pred = weak_learner.predict(X).reshape(num_inputs)\n",
    "      #print(\"y_pred\", y_pred)\n",
    "\n",
    "      e_t = np.sum(self.data_weights * np.abs((y-y_pred))) / np.sum(self.data_weights)\n",
    "      e_t = np.clip(e_t, 1e-10, 0.49)\n",
    "      #e_t = e_t / num_inputs\n",
    "      #print(\"weighted error\", e_t)\n",
    "      \n",
    "      w_t = 0.5 * np.log((1 - e_t) / e_t)\n",
    "      #print(\"w_t\", w_t)\n",
    "\n",
    "      self.estimator_weights[i] = w_t\n",
    "    \n",
    "      self.data_weights *= np.exp(-w_t * np.abs(y-y_pred))\n",
    " \n",
    "      self.data_weights = np.clip(self.data_weights, a_min=1e-10, a_max=1e2)\n",
    "      self.data_weights /= np.sum(self.data_weights)\n",
    "      #print(self.data_weights)\n",
    "      #print(f\"Sum of data weights (should be 1): {np.sum(self.data_weights)}\")  \n",
    "    \n",
    "    self.estimator_weights /= np.sum(self.estimator_weights)\n",
    "    print(self.estimator_weights)\n",
    "\n",
    "\n",
    "  def loss(self, X, Y):\n",
    "      # Get predictions from all learners, then weight them\n",
    "      #print(\"Shape of Y\", Y.shape)\n",
    "      #print(\"one prediction\", self.estimators[0].predict(X).shape)\n",
    "      predictions = np.array([e.predict(X).reshape(-1) for e in self.estimators])\n",
    "      #print(\"prediction shape\", predictions.shape)\n",
    "      #predictions = predictions.reshape(self.n_estimators, Y.shape[0])\n",
    "      #print(\"prediction shape\", predictions.shape)\n",
    "      #print(\"Original Pred\", predictions)\n",
    "      #print(\"Estimator Weights\", self.estimator_weights)\n",
    "      weighted_predictions = np.dot(self.estimator_weights, predictions)\n",
    "      #print(\"Weighted\", weighted_predictions)\n",
    "      #print(Y)\n",
    "      # L2 loss\n",
    "      loss = np.mean((Y - weighted_predictions) ** 2)\n",
    "      return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def test_OneLayerNN():\n",
    "    '''\n",
    "    Tests for OneLayerNN Model Weights Gradient\n",
    "    '''\n",
    "\n",
    "    test_model = OneLayerNN()\n",
    "\n",
    "    # Creates Test Data\n",
    "    x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "    y = np.array([0,0,1,1,0])\n",
    "    no_weights = [1, 1, 1, 1, 1]\n",
    "\n",
    "    # Test Model Train \n",
    "    test_model.fit(x_bias, y, no_weights)\n",
    "\n",
    "    act_weights = test_model.weights\n",
    "    exp_weights = np.array([[ 0.17817953, -0.03543112,  0.34761945]])\n",
    "\n",
    "    print('----Testing 1-Layer NN Gradients-----')\n",
    "\n",
    "    print(\"\\nTesting layer one weights gradient.\")\n",
    "    \n",
    "    # Test layer 1 weights\n",
    "    if not hasattr(act_weights, \"shape\"):\n",
    "        print(\"Layer one weights gradient is not a numpy array. \\n\")\n",
    "    elif act_weights.shape != (1, 3):\n",
    "        print(\n",
    "            f\"Incorrect shape for layer one weights gradient.\\nExpected: {(1, 3)} \\nActual: {act_weights.shape} \\n\")\n",
    "    elif not act_weights == pytest.approx(exp_weights, .01):\n",
    "        print(\n",
    "            f\"Incorrect values for layer one weights gradient.\\nExpected: {exp_weights} \\nActual: {act_weights} \\n\")\n",
    "    else:\n",
    "        print(\"Layer one weights gradient is correct.\\n\")\n",
    "\n",
    "    print('----Testing 1-Layer NN with Data Weights-----')\n",
    "\n",
    "    data_weights_1 = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "    data_weights_2 = [0, 0, 0, 0, 1]\n",
    "\n",
    "    test_model1 = OneLayerNN()\n",
    "    test_model2 = OneLayerNN()\n",
    "    # Test Model Train \n",
    "    test_model1.loss(x_bias, y, data_weights_1)\n",
    "    test_model2.loss(x_bias, y, data_weights_2)\n",
    "\n",
    "    act_weights1 = test_model1.weights\n",
    "    act_weights2 = test_model2.weights\n",
    "    exp_weights1 = np.array([[ 0.17817953, -0.03543112,  0.34761945]])\n",
    "    exp_weights2 = np.array([[ 0.17817953, -0.03543112,  0.34761945]])\n",
    "\n",
    "    if not hasattr(act_weights, \"shape\"):\n",
    "        print(\"Layer one weights gradient is not a numpy array. \\n\")\n",
    "    elif act_weights.shape != (1, 3):\n",
    "        print(\n",
    "            f\"Incorrect shape for layer one weights gradient.\\nExpected: {(1, 3)} \\nActual: {act_weights.shape} \\n\")\n",
    "    elif not act_weights1 == pytest.approx(exp_weights1, .01):\n",
    "        print(\n",
    "            f\"Incorrect values for layer one weights gradient.\\nExpected: {exp_weights} \\nActual: {act_weights} \\n\")\n",
    "    elif not act_weights2 == pytest.approx(exp_weights2, .01):\n",
    "        print(\n",
    "            f\"Incorrect values for layer one weights gradient.\\nExpected: {exp_weights} \\nActual: {act_weights} \\n\")\n",
    "    else:\n",
    "        print(\"Layer one weights gradient with weighted data is correct.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing for Boosted_NN Model Correctness-----\n",
      "[0.1976029  0.22514664 0.1886042  0.21521748 0.17342877]\n",
      "[0.22150789 0.25435303 0.15891299 0.17299408 0.19223201]\n",
      "[0.22050478 0.24221717 0.14996931 0.18726946 0.20003929]\n",
      "[0.2205281  0.24874534 0.12844972 0.20744625 0.19483059]\n",
      "[0.22829586 0.26108798 0.10048409 0.21120522 0.19892685]\n",
      "[0.250047   0.27329996 0.07312588 0.20710207 0.19642509]\n",
      "[0.24700108 0.26711912 0.07856544 0.21121058 0.19610379]\n",
      "[0.24226613 0.29051439 0.08456708 0.20918773 0.17346467]\n",
      "[0.24547844 0.31819651 0.06168021 0.21204333 0.1626015 ]\n",
      "[0.25675394 0.32968298 0.0573663  0.18451233 0.17168445]\n",
      "\n",
      "Testing loss computation for Boosted_NN.\n",
      "Loss computation is correct.\n",
      "\n",
      "Testing average loss computation for Boosted_NN.\n",
      "Average loss computation is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n"
     ]
    }
   ],
   "source": [
    "def test_Boosted_NN():\n",
    "\n",
    "    print('----Testing for Boosted_NN Model Correctness-----')\n",
    "\n",
    "    np.random.seed(4)\n",
    "    random.seed(4)\n",
    "\n",
    "    boosted_model = Boosted_Model(\"nn\", n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "\n",
    "    X_test = np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]])\n",
    "    Y_test = np.array([0, 0, 1, 1, 0])\n",
    "\n",
    "    boosted_model.train(X_test, Y_test)\n",
    "\n",
    "    computed_loss = boosted_model.loss(X_test, Y_test)\n",
    "    expected_loss = 0.02\n",
    "\n",
    "    print(\"\\nTesting loss computation for Boosted_NN.\")\n",
    "    if not isinstance(computed_loss, float):\n",
    "        print(\"Loss is not a float.\")\n",
    "    elif not np.isclose(computed_loss, expected_loss, atol=0.01):\n",
    "        print(f\"Incorrect loss computation.\\nExpected: {expected_loss}\\nActual: {computed_loss}\")\n",
    "    else:\n",
    "        print(\"Loss computation is correct.\")\n",
    "\n",
    "    computed_avg_loss = boosted_model.loss(X_test, Y_test)\n",
    "    expected_avg_loss = computed_loss / X_test.shape[0]\n",
    "\n",
    "    print(\"\\nTesting average loss computation for Boosted_NN.\")\n",
    "    if not isinstance(computed_avg_loss, float):\n",
    "        print(\"Average loss is not a float.\")\n",
    "    elif not np.isclose(computed_avg_loss, expected_avg_loss, atol=0.01):\n",
    "        print(f\"Incorrect average loss computation.\\nExpected: {expected_avg_loss}\\nActual: {computed_avg_loss}\")\n",
    "    else:\n",
    "        print(\"Average loss computation is correct.\")\n",
    "\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test).reshape(-1)  \n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test.shape[0])}\\n\"\n",
    "              f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")\n",
    "\n",
    "test_Boosted_NN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show that it can be reproduced by sklearn's AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- 1-Layer Stumps -----\n",
      "Average Training Loss: 0.5676642741638903\n",
      "Average Testing Loss: 0.7086282707122337\n",
      "----- Our Boosted Network (Decision Stumps) -----\n",
      "Num Estimators:  100\n",
      "Learning Rate:  1\n",
      "[5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.69281280e-05 5.69281280e-05\n",
      " 5.69281280e-05 5.69281280e-05 5.93897555e-05 7.62984442e-05\n",
      " 9.78355674e-05 1.25162551e-04 1.59679966e-04 2.03058596e-04\n",
      " 2.57277228e-04 3.24682569e-04 4.08105198e-04 5.11096129e-04\n",
      " 6.38396883e-04 7.96832862e-04 9.96953455e-04 1.25599883e-03\n",
      " 1.60327949e-03 2.11688546e-03 2.86815314e-03 4.02408016e-03\n",
      " 5.87236263e-03 8.73603370e-03 1.30883775e-02 1.95171530e-02\n",
      " 2.12347078e-02 2.12347126e-02 2.12347127e-02 2.12347130e-02\n",
      " 2.12347127e-02 2.12347125e-02 2.12347127e-02 2.12347127e-02\n",
      " 2.12347125e-02 2.12347123e-02 2.12347130e-02 2.12347126e-02\n",
      " 2.12347126e-02 2.12347127e-02 2.12347128e-02 2.12347126e-02\n",
      " 2.12347126e-02 2.12347128e-02 2.12347127e-02 2.12347122e-02\n",
      " 2.12347125e-02 2.12347126e-02 2.12347127e-02 2.12347129e-02\n",
      " 2.12347124e-02 2.12347124e-02 2.12347128e-02 2.12347128e-02\n",
      " 2.12347124e-02 2.12347129e-02 2.12347128e-02 2.12347125e-02\n",
      " 2.12347126e-02 2.12347129e-02 2.12347130e-02 2.12347124e-02\n",
      " 2.12347124e-02 2.12347125e-02 2.12347125e-02 2.12347124e-02\n",
      " 2.12347124e-02 2.12347124e-02 2.12347127e-02 2.12347126e-02]\n",
      "Average Training Loss: 0.7015694726410191\n",
      "Average Testing Loss: 0.8140573578603847\n",
      "----- sklearn Boosted Network (Decision Stumps) -----\n",
      "Average Training Loss: 0.5321037840337842\n",
      "Average Testing Loss: 0.6608012973514865\n",
      "----- 1-Layer NN -----\n",
      "Average Training Loss: 0.5456180366500258\n",
      "Average Testing Loss: 0.6659666233517516\n",
      "----- Boosted Neural Network -----\n",
      "Num Estimators:  25\n",
      "Learning Rate:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "\n",
    "def test_sklearn(dataset, test_size=0.2):\n",
    "    '''\n",
    "        Tests OneLayerNN, Boost on a given dataset.\n",
    "        :param dataset The path to the dataset\n",
    "        :return None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print('The file {} does not exist'.format(dataset))\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows = 1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    Y = Y \n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    " \n",
    "\n",
    "    #### 1 Layer Stumps ######\n",
    "    print('----- 1-Layer Stumps -----')\n",
    "    nnmodel = DecisionTreeRegressor(max_depth=2)\n",
    "    nnmodel.fit(X_train, Y_train, np.ones(X_train.shape[0]))\n",
    "    print('Average Training Loss:', mean_squared_error(nnmodel.predict(X_train), Y_train))\n",
    "    print('Average Testing Loss:', mean_squared_error(nnmodel.predict(X_test), Y_test))\n",
    "\n",
    "    #### Our Boosted Model with Decision Stumps ######\n",
    "    print('----- Our Boosted Network (Decision Stumps) -----')\n",
    "    n_estimators = 100\n",
    "    learning = 1\n",
    "    print(\"Num Estimators: \", n_estimators)\n",
    "    print(\"Learning Rate: \", learning)\n",
    "    model = Boosted_Model(\"decision stumps\", n_estimators=n_estimators, learning_rate=learning)\n",
    "\n",
    "    model.train(X_train, Y_train)\n",
    "\n",
    "\n",
    "    print('Average Training Loss:', model.loss(X_train, Y_train))\n",
    "    print('Average Testing Loss:', model.loss(X_test, Y_test))\n",
    "\n",
    "    #### sklearn Boosted Decision Stump ######\n",
    "    print('----- sklearn Boosted Network (Decision Stumps) -----')\n",
    "\n",
    "    model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=2),n_estimators=n_estimators, learning_rate=learning)\n",
    "    model.fit(X_train, Y_train)\n",
    "  \n",
    "    print('Average Training Loss:', mean_squared_error(model.predict(X_train), Y_train))\n",
    "    print('Average Testing Loss:', mean_squared_error(model.predict(X_test), Y_test))\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.fit(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0])))\n",
    "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0])))\n",
    "\n",
    "    #### Boosted Neural Networks ######\n",
    "    print('----- Boosted Neural Network -----')\n",
    "    n_estimators = 25\n",
    "    learning = 0.5\n",
    "    print(\"Num Estimators: \", n_estimators)\n",
    "    print(\"Learning Rate: \", learning)\n",
    "\n",
    "    model = Boosted_Model(\"nn\", n_estimators=n_estimators, learning_rate=learning)\n",
    "\n",
    "    model.train(X_train_b, Y_train)\n",
    "\n",
    "    #print(X_train_b.shape)\n",
    "    \n",
    "    # this line errors \n",
    "    # Y_train = Y_train.reshape(-1,1)\n",
    "    # Y_test = Y_test.reshape(-1,1)\n",
    "\n",
    "    print('Average Training Loss:', model.loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', model.loss(X_test_b, Y_test))\n",
    "\n",
    "\n",
    "    #### sklearn Boosted Decision Stump ######\n",
    "    print('----- sklearn Boosted Network (Our NN) -----')\n",
    "\n",
    "    model = AdaBoostRegressor(OneLayerNN(),n_estimators=n_estimators, learning_rate=learning)\n",
    "    model.fit(X_train_b, Y_train)\n",
    "  \n",
    "    print('Average Training Loss:', mean_squared_error(model.predict(X_train_b), Y_train))\n",
    "    print('Average Testing Loss:', mean_squared_error(model.predict(X_test_b), Y_test))\n",
    "\n",
    "test_sklearn('wine.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVjphpjYOI1F"
   },
   "source": [
    "## Accuracy on Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "-mPAx8l8ONaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- 1-Layer NN -----\n",
      "Average Training Loss: 0.5836591698111698\n",
      "Average Testing Loss: 0.5297709239159929\n",
      "----- Boosted Neural Network -----\n",
      "[0.00025583 0.00025697 0.00025722 ... 0.0002559  0.00025646 0.00024615]\n",
      "[0.00025628 0.00025849 0.00025925 ... 0.00025649 0.00025771 0.00023744]\n",
      "[0.0002569  0.00026063 0.00026124 ... 0.00025722 0.00025885 0.00022881]\n",
      "[0.00025721 0.00026283 0.00026338 ... 0.00025779 0.00026    0.00022047]\n",
      "[0.00025744 0.00026494 0.00026549 ... 0.00025835 0.00026115 0.00021246]\n",
      "[0.0002577  0.00026692 0.00026758 ... 0.00025887 0.00026231 0.00020475]\n",
      "[0.00025798 0.00026827 0.00026957 ... 0.00025937 0.00026351 0.00019744]\n",
      "[0.00025824 0.00027014 0.00027153 ... 0.00025993 0.0002646  0.00019021]\n",
      "[0.00025831 0.00027202 0.00027361 ... 0.00026035 0.00026569 0.00018322]\n",
      "[0.00025865 0.00027428 0.00027559 ... 0.000261   0.00026673 0.00017643]\n",
      "[0.0002589  0.00027594 0.00027754 ... 0.0002615  0.00026782 0.00017   ]\n",
      "[0.00025888 0.0002775  0.00027957 ... 0.00026182 0.00026891 0.00016374]\n",
      "[0.00025922 0.00027934 0.00028146 ... 0.00026241 0.00026993 0.00015772]\n",
      "[0.00025926 0.000281   0.00028346 ... 0.00026277 0.00027097 0.00015191]\n",
      "[0.00025945 0.00028282 0.00028534 ... 0.0002633  0.00027194 0.00014626]\n",
      "[0.00025953 0.00028477 0.00028727 ... 0.00026376 0.0002729  0.00014079]\n",
      "[0.00025963 0.00028673 0.00028916 ... 0.00026423 0.00027383 0.00013549]\n",
      "[0.00025958 0.00028899 0.00029112 ... 0.00026466 0.00027474 0.00013034]\n",
      "[0.00025947 0.00029079 0.00029308 ... 0.00026496 0.00027565 0.00012543]\n",
      "[0.00025933 0.0002922  0.00029495 ... 0.00026528 0.00027661 0.00012075]\n",
      "[0.00025926 0.00029404 0.00029682 ... 0.00026566 0.00027749 0.00011618]\n",
      "[0.00025898 0.00029578 0.00029877 ... 0.00026591 0.00027839 0.00011178]\n",
      "[0.00025876 0.00029774 0.00030067 ... 0.00026623 0.00027924 0.0001075 ]\n",
      "[0.00025862 0.00029997 0.00030262 ... 0.00026659 0.00028007 0.00010338]\n",
      "[2.58336217e-04 3.01341797e-04 3.04480632e-04 ... 2.66784934e-04\n",
      " 2.80940481e-04 9.94639823e-05]\n",
      "Average Training Loss: 0.5804192781564804\n",
      "Average Testing Loss: 0.5249426181912324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "\n",
    "\n",
    "def test_models(dataset, test_size=0.2):\n",
    "    '''\n",
    "        Tests OneLayerNN, Boost on a given dataset.\n",
    "        :param dataset The path to the dataset\n",
    "        :return None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print('The file {} does not exist'.format(dataset))\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows = 1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    Y = Y \n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.fit(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0])))\n",
    "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0])))\n",
    "\n",
    "    #### Boosted Neural Networks ######\n",
    "    print('----- Boosted Neural Network -----')\n",
    "    model = Boosted_Model(\"nn\", n_estimators=25)\n",
    "\n",
    "    model.train(X_train_b, Y_train)\n",
    "\n",
    "    #print(X_train_b.shape)\n",
    "    \n",
    "    # this line errors \n",
    "    # Y_train = Y_train.reshape(-1,1)\n",
    "    # Y_test = Y_test.reshape(-1,1)\n",
    "\n",
    "    print('Average Training Loss:', model.loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', model.loss(X_test_b, Y_test))\n",
    "\n",
    "test_models('wine.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 1-Layer NN -----\n",
      "----- Boosted Neural Networks -----\n",
      "Testing Boosted_NN with n_estimators=10, learning_rate=0.01\n",
      "[0.00025399 0.0002545  0.00025769 ... 0.00025794 0.0002568  0.00024763]\n",
      "[0.00025249 0.00025374 0.00026027 ... 0.00026093 0.00025845 0.00023969]\n",
      "[0.00025124 0.00025284 0.00026275 ... 0.00026352 0.00026006 0.00023226]\n",
      "[0.00024964 0.00025201 0.00026549 ... 0.0002664  0.0002618  0.0002244 ]\n",
      "[0.00024812 0.00025114 0.00026817 ... 0.00026933 0.00026351 0.0002169 ]\n",
      "[0.00024674 0.00025026 0.00027072 ... 0.00027212 0.00026506 0.00021009]\n",
      "[0.00024531 0.00024929 0.00027337 ... 0.00027501 0.00026678 0.00020312]\n",
      "[0.00024381 0.00024836 0.00027601 ... 0.00027789 0.00026844 0.00019639]\n",
      "[0.00024261 0.00024746 0.00027844 ... 0.00028031 0.00026991 0.0001906 ]\n",
      "[0.00024096 0.00024659 0.00028117 ... 0.00028304 0.00027161 0.00018406]\n",
      "Training Loss: 0.5679947539387542, Testing Loss: 0.5832947185941474\n",
      "Testing Boosted_NN with n_estimators=10, learning_rate=0.1\n",
      "[0.00025386 0.00025448 0.00025776 ... 0.00025817 0.00025685 0.00024727]\n",
      "[0.00025239 0.00025366 0.00026042 ... 0.00026114 0.00025855 0.00023921]\n",
      "[0.00025109 0.00025274 0.00026297 ... 0.00026382 0.0002602  0.00023163]\n",
      "[0.0002496  0.00025184 0.00026561 ... 0.00026678 0.00026187 0.00022399]\n",
      "[0.00024842 0.00025093 0.00026805 ... 0.00026928 0.0002634  0.00021727]\n",
      "[0.00024703 0.00025004 0.00027059 ... 0.00027205 0.00026497 0.00021042]\n",
      "[0.00024576 0.00024914 0.00027306 ... 0.00027471 0.00026652 0.00020397]\n",
      "[0.00024412 0.00024826 0.00027582 ... 0.00027744 0.00026821 0.00019701]\n",
      "[0.00024264 0.00024732 0.00027847 ... 0.00028046 0.00026985 0.00019052]\n",
      "[0.00024106 0.00024642 0.00028116 ... 0.00028343 0.00027145 0.00018417]\n",
      "Training Loss: 0.5678759974834455, Testing Loss: 0.5831812698540296\n",
      "Testing Boosted_NN with n_estimators=10, learning_rate=0.5\n",
      "[0.00025399 0.00025438 0.00025772 ... 0.00025791 0.00025678 0.00024747]\n",
      "[0.00025246 0.00025359 0.00026038 ... 0.00026081 0.00025847 0.00023935]\n",
      "[0.00025123 0.00025268 0.00026286 ... 0.00026334 0.00026003 0.00023205]\n",
      "[0.00024963 0.00025191 0.00026555 ... 0.00026612 0.00026173 0.00022432]\n",
      "[0.00024817 0.00025108 0.00026812 ... 0.000269   0.00026338 0.00021712]\n",
      "[0.00024676 0.0002502  0.00027067 ... 0.00027181 0.00026493 0.00021027]\n",
      "[0.00024539 0.00024939 0.00027316 ... 0.0002746  0.00026639 0.00020389]\n",
      "[0.00024399 0.00024846 0.00027575 ... 0.00027748 0.00026799 0.00019733]\n",
      "[0.00024271 0.00024751 0.00027825 ... 0.00028    0.00026951 0.00019129]\n",
      "[0.00024119 0.00024659 0.00028088 ... 0.00028306 0.00027114 0.00018496]\n",
      "Training Loss: 0.5665037095083084, Testing Loss: 0.5808668749715679\n",
      "Testing Boosted_NN with n_estimators=50, learning_rate=0.01\n",
      "[0.00025393 0.00025435 0.00025779 ... 0.00025794 0.00025688 0.00024716]\n",
      "[0.0002525  0.00025354 0.00026037 ... 0.00026093 0.00025846 0.00023932]\n",
      "[0.00025116 0.00025268 0.0002629  ... 0.00026353 0.00026008 0.00023182]\n",
      "[0.00024987 0.00025184 0.00026539 ... 0.00026622 0.00026164 0.00022474]\n",
      "[0.00024841 0.00025104 0.00026793 ... 0.00026918 0.0002632  0.00021767]\n",
      "[0.00024709 0.00025011 0.00027047 ... 0.00027184 0.0002648  0.00021078]\n",
      "[0.00024547 0.0002493  0.00027314 ... 0.00027471 0.00026642 0.00020384]\n",
      "[0.00024411 0.00024841 0.00027566 ... 0.00027751 0.00026794 0.00019752]\n",
      "[0.00024247 0.00024758 0.00027836 ... 0.00028034 0.00026964 0.00019084]\n",
      "[0.00024107 0.00024667 0.00028091 ... 0.00028318 0.00027117 0.00018484]\n",
      "[0.00023952 0.00024575 0.00028355 ... 0.00028616 0.00027281 0.00017873]\n",
      "[0.00023816 0.00024484 0.00028606 ... 0.00028895 0.00027429 0.00017321]\n",
      "[0.00023666 0.00024387 0.0002887  ... 0.00029194 0.00027589 0.00016748]\n",
      "[0.00023515 0.00024287 0.00029135 ... 0.00029502 0.00027754 0.00016185]\n",
      "[0.00023376 0.00024189 0.00029389 ... 0.0002979  0.00027908 0.00015668]\n",
      "[0.00023236 0.00024093 0.00029642 ... 0.00030074 0.00028054 0.00015175]\n",
      "[0.00023073 0.00024003 0.00029908 ... 0.00030364 0.00028216 0.00014664]\n",
      "[0.0002292  0.00023907 0.00030167 ... 0.0003067  0.00028371 0.00014182]\n",
      "[0.00022782 0.00023812 0.00030417 ... 0.00030947 0.00028508 0.0001375 ]\n",
      "[0.00022629 0.00023712 0.0003068  ... 0.00031262 0.0002866  0.00013295]\n",
      "[0.00022481 0.0002362  0.00030935 ... 0.00031566 0.00028803 0.00012875]\n",
      "[0.00022331 0.00023513 0.00031197 ... 0.00031862 0.00028955 0.00012446]\n",
      "[0.00022191 0.0002341  0.0003145  ... 0.00032133 0.00029102 0.00012048]\n",
      "[0.00022034 0.0002331  0.00031713 ... 0.0003244  0.0002925  0.00011648]\n",
      "[0.00021884 0.00023211 0.00031972 ... 0.00032742 0.00029391 0.00011271]\n",
      "[0.00021717 0.00023091 0.00032275 ... 0.00033083 0.00029556 0.00010848]\n",
      "[0.00021489 0.00022948 0.00032655 ... 0.00033524 0.0002976  0.00010334]\n",
      "[2.12110561e-04 2.27451779e-04 3.31592845e-04 ... 3.40944092e-04\n",
      " 3.00246640e-04 9.69794442e-05]\n",
      "[2.08280412e-04 2.24901505e-04 3.38084751e-04 ... 3.48321001e-04\n",
      " 3.03839270e-04 8.91340363e-05]\n",
      "[2.03264872e-04 2.21518518e-04 3.46594860e-04 ... 3.58031655e-04\n",
      " 3.08542976e-04 7.97617566e-05]\n",
      "[1.96859811e-04 2.17242749e-04 3.57575074e-04 ... 3.71002337e-04\n",
      " 3.14136475e-04 6.92262616e-05]\n",
      "[1.89048800e-04 2.11641623e-04 3.71435407e-04 ... 3.88265920e-04\n",
      " 3.20951744e-04 5.79074005e-05]\n",
      "[1.78889097e-04 2.04031119e-04 3.89507440e-04 ... 4.10908030e-04\n",
      " 3.30016793e-04 4.55698580e-05]\n",
      "[1.68193598e-04 1.94525162e-04 4.10268096e-04 ... 4.33973843e-04\n",
      " 3.39783944e-04 3.45662761e-05]\n",
      "[1.53258181e-04 1.83671699e-04 4.37967619e-04 ... 4.65233897e-04\n",
      " 3.51142740e-04 2.40079678e-05]\n",
      "[1.36069700e-04 1.69553946e-04 4.73040310e-04 ... 5.03537707e-04\n",
      " 3.65652921e-04 1.50141518e-05]\n",
      "[1.17726333e-04 1.53904179e-04 5.13231896e-04 ... 5.48828948e-04\n",
      " 3.79823775e-04 8.68982711e-06]\n",
      "[1.00148682e-04 1.36028804e-04 5.57842235e-04 ... 6.08913103e-04\n",
      " 3.93379451e-04 4.68864064e-06]\n",
      "[8.12201257e-05 1.15626246e-04 6.12079794e-04 ... 6.81811567e-04\n",
      " 4.06533279e-04 2.17057579e-06]\n",
      "[6.37525876e-05 9.58206386e-05 6.69913102e-04 ... 7.68059960e-04\n",
      " 4.16471612e-04 9.21173416e-07]\n",
      "[4.60991945e-05 7.68027891e-05 7.41387366e-04 ... 8.20885713e-04\n",
      " 4.23591080e-04 3.19405522e-07]\n",
      "[3.16236061e-05 5.86721699e-05 8.18858732e-04 ... 8.86488679e-04\n",
      " 4.24269592e-04 9.58057574e-08]\n",
      "[2.05396856e-05 4.27528518e-05 9.00417396e-04 ... 9.48639903e-04\n",
      " 4.18670430e-04 2.48085528e-08]\n",
      "[1.24732891e-05 2.97098629e-05 9.89007700e-04 ... 9.84182048e-04\n",
      " 4.05980797e-04 5.42677493e-09]\n",
      "[7.39692563e-06 2.00610899e-05 1.07175132e-03 ... 1.06028861e-03\n",
      " 3.83994214e-04 1.15416850e-09]\n",
      "[4.04093469e-06 1.26782116e-05 1.15921600e-03 ... 1.09994826e-03\n",
      " 3.57937444e-04 1.95121301e-10]\n",
      "[2.18977341e-06 7.88366899e-06 1.23698586e-03 ... 1.19631060e-03\n",
      " 3.27005289e-04 1.12658055e-10]\n",
      "[1.04843161e-06 4.68099076e-06 1.34250135e-03 ... 1.14052124e-03\n",
      " 2.97960987e-04 1.12253048e-10]\n",
      "[5.36084342e-07 2.69666915e-06 1.41930972e-03 ... 1.26942292e-03\n",
      " 2.65988129e-04 1.11878708e-10]\n",
      "[2.33337290e-07 1.45764621e-06 1.50264821e-03 ... 1.24279804e-03\n",
      " 2.31147613e-04 1.10938112e-10]\n",
      "Training Loss: 0.5705943915147457, Testing Loss: 0.5858691352375045\n",
      "Testing Boosted_NN with n_estimators=50, learning_rate=0.1\n",
      "[0.00025399 0.00025439 0.00025771 ... 0.00025785 0.00025683 0.00024739]\n",
      "[0.00025275 0.00025352 0.0002602  ... 0.00026047 0.00025842 0.00023982]\n",
      "[0.00025135 0.00025267 0.00026275 ... 0.00026325 0.00026007 0.00023216]\n",
      "[0.00025011 0.00025182 0.00026521 ... 0.00026585 0.00026161 0.00022516]\n",
      "[0.00024862 0.00025105 0.00026777 ... 0.00026886 0.00026321 0.00021798]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_models_with_hyperparameters(dataset, test_size=0.2, n_estimators_list=[10, 50, 100], learning_rate_list=[0.01, 0.1, 0.5]):\n",
    "    '''\n",
    "        Tests OneLayerNN and Boosted_NN with varying hyperparameters.\n",
    "        :param dataset: The path to the dataset\n",
    "        :param test_size: Fraction of the dataset to be used for testing\n",
    "        :param n_estimators_list: List of values for the number of estimators\n",
    "        :param learning_rate_list: List of values for the learning rate\n",
    "        :return: None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f'The file {dataset} does not exist')\n",
    "        return\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Add a bias term to the features\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    # Results storage for plotting\n",
    "    nn_train_losses = []\n",
    "    nn_test_losses = []\n",
    "    boosted_train_losses = {}\n",
    "    boosted_test_losses = {}\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.fit(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    nn_train_loss = nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    nn_test_loss = nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0]))\n",
    "    nn_train_losses.append(nn_train_loss)\n",
    "    nn_test_losses.append(nn_test_loss)\n",
    "    # print('Average Training Loss (1-Layer NN):', nn_train_loss)\n",
    "    # print('Average Testing Loss (1-Layer NN):', nn_test_loss)\n",
    "\n",
    "    #### Boosted Neural Networks ######\n",
    "    print('----- Boosted Neural Networks -----')\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            print(f'Testing Boosted_NN with n_estimators={n_estimators}, learning_rate={learning_rate}')\n",
    "            model = Boosted_Model(\"nn\", n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "            model.train(X_train_b, Y_train)\n",
    "            train_loss = model.loss(X_train_b, Y_train)\n",
    "            test_loss = model.loss(X_test_b, Y_test)\n",
    "            print(f'Training Loss: {train_loss}, Testing Loss: {test_loss}')\n",
    "\n",
    "            # Store results\n",
    "            boosted_train_losses[(n_estimators, learning_rate)] = train_loss\n",
    "            boosted_test_losses[(n_estimators, learning_rate)] = test_loss\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot for Boosted NN\n",
    "    for n_estimators in n_estimators_list:\n",
    "        train_losses = [boosted_train_losses[(n_estimators, lr)] for lr in learning_rate_list]\n",
    "        test_losses = [boosted_test_losses[(n_estimators, lr)] for lr in learning_rate_list]\n",
    "        \n",
    "        ax.plot(learning_rate_list, train_losses, marker='o', label=f'Train Loss (n_estimators={n_estimators})')\n",
    "        ax.plot(learning_rate_list, test_losses, marker='x', label=f'Test Loss (n_estimators={n_estimators})', linestyle='--')\n",
    "    \n",
    "    ax.set_title('Boosted NN Loss vs Learning Rate')\n",
    "    ax.set_xlabel('Learning Rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xscale('log')  # Keep the log scale for learning rates\n",
    "    ax.set_xticks(learning_rate_list)  # Explicitly set the ticks\n",
    "    ax.get_xaxis().set_major_formatter(plt.ScalarFormatter())  # Ensure proper formatting of tick labels\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "test_models_with_hyperparameters('wine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing for Boosted_NN Model Correctness-----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeak learner predictions shape is correct.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtest_Boosted_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 13\u001b[0m, in \u001b[0;36mtest_Boosted_NN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     11\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m \u001b[43mboosted_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m computed_loss \u001b[38;5;241m=\u001b[39m boosted_model\u001b[38;5;241m.\u001b[39mloss(X_test, Y_test)\n\u001b[1;32m     16\u001b[0m expected_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n",
      "Cell \u001b[0;32mIn[53], line 35\u001b[0m, in \u001b[0;36mBoosted_Model.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators[i]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Fit the weak learner\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mweak_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(self.data_weights)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#print(\"Before Reshape\", weak_learner.predict(X).shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m weak_learner\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mreshape(num_inputs)\n",
      "Cell \u001b[0;32mIn[52], line 67\u001b[0m, in \u001b[0;36mOneLayerNN.fit\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m     65\u001b[0m num_examples, num_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, num_features))\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     data_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(num_examples)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing for Boosted_NN vs Weak Learner-----\n",
      "\n",
      "---Testing on Synthetic small dataset---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoosted model does not improve in average loss over the weak learner.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mtest_Boosted_vs_WeakLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 38\u001b[0m, in \u001b[0;36mtest_Boosted_vs_WeakLearner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m OneLayerNN()\n\u001b[1;32m     36\u001b[0m data_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(Y_test)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_test)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mboosted_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m weak_learner\u001b[38;5;241m.\u001b[39mfit(X_test, Y_test, data_weights)\n\u001b[1;32m     42\u001b[0m boosted_loss \u001b[38;5;241m=\u001b[39m boosted_model\u001b[38;5;241m.\u001b[39mloss(X_test, Y_test)\n",
      "Cell \u001b[0;32mIn[53], line 35\u001b[0m, in \u001b[0;36mBoosted_Model.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators[i]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Fit the weak learner\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mweak_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(self.data_weights)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#print(\"Before Reshape\", weak_learner.predict(X).shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m weak_learner\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mreshape(num_inputs)\n",
      "Cell \u001b[0;32mIn[52], line 67\u001b[0m, in \u001b[0;36mOneLayerNN.fit\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m     65\u001b[0m num_examples, num_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, num_features))\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     data_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(num_examples)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def test_Boosted_vs_WeakLearner():\n",
    "\n",
    "    print('----Testing for Boosted_NN vs Weak Learner-----')\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"X_test\": np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]]),\n",
    "            \"Y_test\": np.array([0, 0, 1, 1, 0]),\n",
    "            \"description\": \"Synthetic small dataset\"\n",
    "        },\n",
    "        {\n",
    "            \"X_test\": np.array([[2, 1, 3], [1, 2, 1], [3, 3, 3], [4, 0, 2], [5, 1, 1]]),\n",
    "            \"Y_test\": np.array([1, 0, 1, 1, 0]),\n",
    "            \"description\": \"Synthetic dataset with varying inputs\"\n",
    "        },\n",
    "        {\n",
    "            \"X_test\": np.array([[0, 0, 1], [1, 1, 1], [2, 2, 1], [3, 3, 1], [4, 4, 1]]),\n",
    "            \"Y_test\": np.array([1, 2, 3, 4, 5]),\n",
    "            \"description\": \"Synthetic regression dataset\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for idx, case in enumerate(test_cases):\n",
    "        X_test = case[\"X_test\"]\n",
    "        Y_test = case[\"Y_test\"]\n",
    "        description = case[\"description\"]\n",
    "\n",
    "        print(f\"\\n---Testing on {description}---\")\n",
    "\n",
    "        np.random.seed(idx+4)\n",
    "        random.seed(idx+4)\n",
    "\n",
    "        boosted_model = Boosted_Model(\"nn\", n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "        weak_learner = OneLayerNN()\n",
    "\n",
    "        data_weights = np.ones(len(Y_test)) / len(Y_test)\n",
    "\n",
    "        boosted_model.train(X_test, Y_test)\n",
    "\n",
    "        weak_learner.fit(X_test, Y_test, data_weights)\n",
    "\n",
    "        boosted_loss = boosted_model.loss(X_test, Y_test)\n",
    "        weak_loss = weak_learner.loss(X_test, Y_test, data_weights)\n",
    "\n",
    "        print(f\"Weak Learner Loss: {weak_loss}\")\n",
    "        print(f\"Boosted Model Loss: {boosted_loss}\")\n",
    "\n",
    "        if boosted_loss < weak_loss:\n",
    "            print(\"Boosted model performs better than the weak learner.\")\n",
    "        else:\n",
    "            print(\"Boosted model does not improve over the weak learner.\")\n",
    "\n",
    "        boosted_avg_loss = boosted_model.loss(X_test, Y_test)\n",
    "        weak_avg_loss = weak_learner.average_loss(X_test, Y_test, data_weights)\n",
    "\n",
    "        print(f\"Weak Learner Average Loss: {weak_avg_loss}\")\n",
    "        print(f\"Boosted Model Average Loss: {boosted_avg_loss}\")\n",
    "\n",
    "        if boosted_avg_loss < weak_avg_loss:\n",
    "            print(\"Boosted model has lower average loss than the weak learner.\")\n",
    "        else:\n",
    "            print(\"Boosted model does not improve in average loss over the weak learner.\")\n",
    "\n",
    "test_Boosted_vs_WeakLearner()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Against the sklearn implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- 1-Layer Stumps -----\n",
      "Average Training Loss: 0.6505357886168747\n",
      "Average Testing Loss: 0.6875904538423815\n",
      "----- Our Boosted Network (Decision Stumps) -----\n",
      "Num Estimators:  100\n",
      "Learning Rate:  1\n",
      "Average Training Loss: 0.7023261831185049\n",
      "Average Testing Loss: 0.719118753024704\n",
      "----- sklearn Boosted Network (Decision Stumps) -----\n",
      "Average Training Loss: 0.5631981193160046\n",
      "Average Testing Loss: 0.6015461722875634\n",
      "----- 1-Layer NN -----\n",
      "Average Training Loss: 0.5614259097798575\n",
      "Average Testing Loss: 0.5985943946824918\n",
      "----- Boosted Neural Network -----\n",
      "Num Estimators:  25\n",
      "Learning Rate:  0.5\n",
      "Average Training Loss: 0.5566066189193396\n",
      "Average Testing Loss: 0.5962706598563772\n",
      "----- sklearn Boosted Network (Our NN) -----\n",
      "Average Training Loss: 0.5659198266971857\n",
      "Average Testing Loss: 0.6042774115349053\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO they should be in the Harvard Citation Format apparently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "y7jqYzqTDwBc"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

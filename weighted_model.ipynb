{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIZqWCd2L6C"
      },
      "source": [
        "# Boosting With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJqYh6u2Qtz"
      },
      "source": [
        "## Markdown Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppdKF2TvGOLo"
      },
      "source": [
        "# Representation\n",
        "\n",
        "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
        "\n",
        "$$E(H, T) = {x \\to sign(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUF8LgSz3Nna"
      },
      "source": [
        "### Check Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKoyt0u2tiR",
        "outputId": "357f42eb-bd03-4dcb-a75b-cd289e2a5cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.5\n",
            "\n",
            "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from packaging.version import parse as Version\n",
        "from platform import python_version\n",
        "\n",
        "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
        "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "except ImportError:\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % sys.version)\n",
        "\n",
        "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
        "    mod = None\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        if pkg in {'PIL'}:\n",
        "            ver = mod.VERSION\n",
        "        else:\n",
        "            ver = mod.__version__\n",
        "        if Version(ver) == Version(min_ver):\n",
        "            print(OK, \"%s version %s is installed.\"\n",
        "                  % (lib, min_ver))\n",
        "        else:\n",
        "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
        "                  % (lib, min_ver, ver))\n",
        "    except ImportError:\n",
        "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
        "    return mod\n",
        "\n",
        "\n",
        "# first check the python version\n",
        "pyversion = Version(python_version())\n",
        "\n",
        "if pyversion >= Version(\"3.12.5\"):\n",
        "    print(OK, \"Python version is %s\" % pyversion)\n",
        "elif pyversion < Version(\"3.12.5\"):\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % pyversion)\n",
        "else:\n",
        "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
        "\n",
        "\n",
        "print()\n",
        "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\",\n",
        "                'pandas': \"2.2.2\"}\n",
        "\n",
        "# now the dependencies\n",
        "for lib, required_version in list(requirements.items()):\n",
        "    import_version(lib, required_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfCnqsOY3WLp"
      },
      "source": [
        "## Model Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npX8gMU3Y0I"
      },
      "source": [
        "### Weak Learner: One Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dFNBypLJ1nJY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def l2_loss_weight(predictions,Y,weights):\n",
        "    '''\n",
        "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
        "        that are weighted\n",
        "        :param Y: A 1D Numpy array with real values (float64)\n",
        "        :param predictions: A 1D Numpy array of the same size of Y\n",
        "        :param weights: A 1D Numpy array of the same size of Y,\n",
        "        :return: Weighted L2 loss using predictions for Y.\n",
        "    '''\n",
        "\n",
        "    return np.sum(weights * (predictions - Y)**2)\n",
        "\n",
        "class OneLayerNN:\n",
        "    '''\n",
        "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        @attrs:\n",
        "            weights: The weights of the neural network model.\n",
        "            batch_size: The number of examples in each batch\n",
        "            learning_rate: The learning rate to use for SGD\n",
        "            epochs: The number of times to pass through the dataset\n",
        "            v: The resulting predictions computed during the forward pass\n",
        "        '''\n",
        "        # initialize self.weights in train()\n",
        "        self.weights = None\n",
        "        self.learning_rate = 0.001\n",
        "        self.epochs = 25\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # initialize self.v in forward_pass()\n",
        "        self.v = None\n",
        "\n",
        "    def train(self, X, Y, data_weights, print_loss=False):\n",
        "        '''\n",
        "        Trains the OneLayerNN model using SGD.\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :param print_loss: If True, print the loss after each epoch.\n",
        "        :return: None\n",
        "        '''\n",
        "        # Initialize weights\n",
        "        input_size = X.shape[1]\n",
        "        self.weights = np.random.uniform(0,1,(1, input_size))\n",
        "        #print(\"Weights Initial Shape: \", self.weights.shape)\n",
        "\n",
        "        # Train network for certain number of epochs\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # Shuffle the examples (X) and labels (Y)\n",
        "            rand_index = np.arange(X.shape[0])\n",
        "            np.random.shuffle(rand_index)\n",
        "            X_s = X[rand_index]\n",
        "            Y_s = Y[rand_index]\n",
        "            data_weights_s = data_weights[rand_index]\n",
        "\n",
        "             # iterate through the examples in batch size increments\n",
        "            for i in range((int(np.ceil(X_s.shape[0] / self.batch_size)))):\n",
        "                X_batch = X_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "                Y_batch = Y_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "                data_weights_batch = data_weights_s[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "\n",
        "\n",
        "                #Perform the forward and backward pass on the current batch\n",
        "                self.forward_pass(X_batch)\n",
        "                self.backward_pass(X_batch, Y_batch, data_weights_batch)\n",
        "\n",
        "            # Print the loss after every epoch\n",
        "            if print_loss:\n",
        "                print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y, data_weights_batch)))\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        '''\n",
        "        Computes the predictions for a single layer given examples X and\n",
        "        stores them in self.v\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :return: None\n",
        "        '''\n",
        "\n",
        "        self.v = np.dot(self.weights, X.T) # + bias (no bias for now?)\n",
        "        #print(\"v shape: \", self.v.shape)\n",
        "\n",
        "\n",
        "\n",
        "    def backward_pass(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Computes the weights gradient and updates self.weights\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: None\n",
        "        '''\n",
        "        # Compute the gradients for the model's weights using backprop\n",
        "        grads = self.backprop(X,Y, data_weights)\n",
        "\n",
        "        # Update the weights using gradient descent\n",
        "        self.gradient_descent(grads)\n",
        "\n",
        "\n",
        "\n",
        "    def backprop(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the average weights gradient for the given batch\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A 1D Numpy array representing the weights gradient\n",
        "        '''\n",
        "        # Compute the average weights gradient\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "\n",
        "        # The gradient dL/dw = -2 * xi * predictons (sum(yi-h(xi)))\n",
        "        m = X.shape[0]\n",
        "        grad_W = (2/m) * np.dot(X.T, data_weights * (self.v - Y))\n",
        "\n",
        "        #print(\"Shape of grad_W: \", grad_W.T.shape)\n",
        "\n",
        "        return grad_W.T\n",
        "\n",
        "    def gradient_descent(self, grad_W):\n",
        "        '''\n",
        "        Updates the weights using the given gradient\n",
        "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
        "        :return: None\n",
        "        '''\n",
        "        # Update the weights using the given gradient and the learning rate\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "        self.weights = self.weights - (self.learning_rate * grad_W)\n",
        "        #print(self.weights)\n",
        "\n",
        "\n",
        "    def loss(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the total squared error on some dataset (X, Y).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the squared error of the model on the dataset\n",
        "        '''\n",
        "        # Perform the forward pass and compute the l2 loss\n",
        "        self.forward_pass(X)\n",
        "        return l2_loss_weight(self.v, Y, data_weights)\n",
        "\n",
        "    def average_loss(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the mean squared error on some dataset (X, Y).\n",
        "        MSE = Total squared error/# of examples\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the mean squared error of the model on the dataset\n",
        "        '''\n",
        "        return self.loss(X, Y, data_weights) / X.shape[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Returns the predicted values for some dataset (X).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :return: 1D Numpy array containing the predicted values for each example\n",
        "        '''\n",
        "        self.forward_pass(X)\n",
        "        return self.v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3JqPeK3hVC"
      },
      "source": [
        "### Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eDK23Q7F3gZJ"
      },
      "outputs": [],
      "source": [
        "class Boosted_NN:\n",
        "  def __init__(self, n_estimators=50, learning_rate=0.01, random_state=1):\n",
        "    self.n_estimators = n_estimators\n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "    self.estimator_weights = np.zeros(self.n_estimators)\n",
        "    self.data_weights = []\n",
        "\n",
        "    # Initialize the estimators\n",
        "    self.estimators = []\n",
        "    for i in range(self.n_estimators):\n",
        "      self.estimators.append(OneLayerNN())\n",
        "\n",
        "  def train(self, X, y):\n",
        "    '''\n",
        "    Trains/Fits the Boosting Model using AdaBoost.\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "    '''\n",
        "    # Initialize the data and estimator weights\n",
        "    num_inputs = X.shape[0]\n",
        "    self.data_weights = np.ones(num_inputs) / num_inputs\n",
        "\n",
        "    # For each round/weak learner\n",
        "    for i in range(self.n_estimators):\n",
        "\n",
        "      # Use the weak learner\n",
        "      weak_learner = self.estimators[i]\n",
        "\n",
        "      # Fit the weak learner\n",
        "      weak_learner.train(X, y, self.data_weights)\n",
        "\n",
        "      y_pred = weak_learner.predict(X).reshape(num_inputs)\n",
        "\n",
        "\n",
        "\n",
        "      e_t = np.sum(self.data_weights * (y-y_pred)**2) / np.sum(self.data_weights) \n",
        "      e_t = e_t / num_inputs\n",
        "      print(\"weighted error\", e_t)\n",
        "      \n",
        "\n",
        "      w_t = 0.5 * np.log((1 / e_t) - 1)\n",
        "      print(\"w_t\", w_t)\n",
        "\n",
        "      self.estimator_weights[i] = w_t\n",
        "      \n",
        "      self.data_weights *= np.exp(-w_t * (y-y_pred))\n",
        "      self.data_weights = np.clip(self.data_weights, a_min=1e-10, a_max=1e5)\n",
        "      self.data_weights /= np.sum(self.data_weights)\n",
        "      # print(f\"Sum of data weights (should be 1): {np.sum(self.data_weights)}\")  \n",
        "\n",
        "\n",
        "  def loss(self, X, Y):\n",
        "      # Get predictions from all learners, then weight them\n",
        "      predictions = np.array([e.predict(X) for e in self.estimators])\n",
        "      weighted_predictions = np.dot(self.estimator_weights, predictions)\n",
        "\n",
        "      # L2 loss\n",
        "      loss = np.mean((Y - weighted_predictions) ** 2)\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    '''\n",
        "    Returns the predicted values for some dataset (X).\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :return: 1D Numpy array containing the predicted values for each example\n",
        "    '''\n",
        "    y_pred = np.array([e.predict(X) for e in self.estimators])\n",
        "    return np.dot(self.estimator_weights, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjphpjYOI1F"
      },
      "source": [
        "## Accuracy on Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-mPAx8l8ONaB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running models on wine.txt dataset\n",
            "----- 1-Layer NN -----\n",
            "Average Training Loss: 0.005685981406105489\n",
            "Average Testing Loss: 0.005579457700915277\n",
            "----- Boosted Neural Network -----\n",
            "weighted error 0.0010455314384331513\n",
            "w_t 3.4310919454613193\n",
            "Sum of data weights (should be 1): 0.9999999999999998\n",
            "weighted error 0.0005453923209994108\n",
            "w_t 3.7567298130630595\n",
            "Sum of data weights (should be 1): 0.9999999999999997\n",
            "weighted error 0.0006364489900651055\n",
            "w_t 3.679484816432292\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.00023163138921231404\n",
            "w_t 4.185065815612175\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.00014578647730082147\n",
            "w_t 4.416610846932805\n",
            "Sum of data weights (should be 1): 0.9999999999999997\n",
            "weighted error 0.0007066003989853806\n",
            "w_t 3.627169204112729\n",
            "Sum of data weights (should be 1): 0.9999999999999997\n",
            "weighted error 0.00025538396414338533\n",
            "w_t 4.136243493489654\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.0003762279320910087\n",
            "w_t 3.942469548573767\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 6.730296623604034e-05\n",
            "w_t 4.803119471111977\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.0010533642760299862\n",
            "w_t 3.4273561225254867\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.0003387347317804151\n",
            "w_t 3.9949772334528677\n",
            "Sum of data weights (should be 1): 1.0000000000000004\n",
            "weighted error 0.0006055772670321882\n",
            "w_t 3.704361316884154\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.0001283243075054898\n",
            "w_t 4.480410756701114\n",
            "Sum of data weights (should be 1): 1.0000000000000004\n",
            "weighted error 0.0011969651948713596\n",
            "w_t 3.363384123832731\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.00120911515987105\n",
            "w_t 3.358328306494183\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.00048279587864189253\n",
            "w_t 3.8177168463412414\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.00016174874705800844\n",
            "w_t 4.3646523044736245\n",
            "Sum of data weights (should be 1): 0.9999999999999996\n",
            "weighted error 0.0005736175499960746\n",
            "w_t 3.7314869453939843\n",
            "Sum of data weights (should be 1): 0.9999999999999997\n",
            "weighted error 0.00023692226060348137\n",
            "w_t 4.173770767318898\n",
            "Sum of data weights (should be 1): 0.9999999999999998\n",
            "weighted error 0.0001292526178686747\n",
            "w_t 4.476806264753026\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.0002982042247060785\n",
            "w_t 4.058716869839912\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.0006120425577806117\n",
            "w_t 3.6990482545400845\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.000419627107437632\n",
            "w_t 3.887862182654204\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.00019450673421755315\n",
            "w_t 4.272424623312771\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.0006848054377963356\n",
            "w_t 3.6428453762184296\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.0005365679068726472\n",
            "w_t 3.7648903590692067\n",
            "Sum of data weights (should be 1): 0.9999999999999994\n",
            "weighted error 0.0008447541891868494\n",
            "w_t 3.5378098810511687\n",
            "Sum of data weights (should be 1): 1.0000000000000004\n",
            "weighted error 0.00010952308079627272\n",
            "w_t 4.559632859156477\n",
            "Sum of data weights (should be 1): 0.9999999999999998\n",
            "weighted error 0.0010134819327779587\n",
            "w_t 3.446674711470848\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.00023217627231254337\n",
            "w_t 4.183890738557815\n",
            "Sum of data weights (should be 1): 0.9999999999999998\n",
            "weighted error 0.0002992713214419747\n",
            "w_t 4.056930325177521\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.00040197361491482524\n",
            "w_t 3.909361025805782\n",
            "Sum of data weights (should be 1): 1.0000000000000004\n",
            "weighted error 0.0005554967804997116\n",
            "w_t 3.747546046727737\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.0001881236230348863\n",
            "w_t 4.289111550291652\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.00042905099870990855\n",
            "w_t 3.8767528124612998\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.0019475751662083314\n",
            "w_t 3.119610355275782\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.0013323506552041866\n",
            "w_t 3.3097386239140323\n",
            "Sum of data weights (should be 1): 0.9999999999999994\n",
            "weighted error 0.0013638804639733392\n",
            "w_t 3.2980282741944293\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.00028643304191314033\n",
            "w_t 4.078859642826113\n",
            "Sum of data weights (should be 1): 0.9999999999999998\n",
            "weighted error 3.8987288513884426e-05\n",
            "w_t 5.076117956228024\n",
            "Sum of data weights (should be 1): 1.0000000000000004\n",
            "weighted error 0.00016604830082365087\n",
            "w_t 4.351532890512702\n",
            "Sum of data weights (should be 1): 1.0000000000000002\n",
            "weighted error 0.0002920424630548755\n",
            "w_t 4.069159629866048\n",
            "Sum of data weights (should be 1): 1.0000000000000004\n",
            "weighted error 0.0006882850781045479\n",
            "w_t 3.6403094630441286\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.0023797264479757134\n",
            "w_t 3.019193586616371\n",
            "Sum of data weights (should be 1): 0.9999999999999996\n",
            "weighted error 0.00042545159952817014\n",
            "w_t 3.8809669119655554\n",
            "Sum of data weights (should be 1): 0.9999999999999994\n",
            "weighted error 0.00026071165203260977\n",
            "w_t 4.1259173986017625\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.000603267589917594\n",
            "w_t 3.7062731228341166\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.00017170215800289503\n",
            "w_t 4.334788752394573\n",
            "Sum of data weights (should be 1): 1.0\n",
            "weighted error 0.001215718408299987\n",
            "w_t 3.355601818191722\n",
            "Sum of data weights (should be 1): 0.9999999999999999\n",
            "weighted error 0.00043062850578143205\n",
            "w_t 3.874917026914807\n",
            "Sum of data weights (should be 1): 1.0\n",
            "(3918, 12)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "shapes (50,) and (50,1,3918) not aligned: 50 (dim 0) != 1 (dim 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Training Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mloss(X_train_b, Y_train))\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Testing Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mloss(X_test_b, Y_test))\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtest_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwine.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mtest_models\u001b[0;34m(dataset, test_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m Y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m Y_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Training Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Testing Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mloss(X_test_b, Y_test))\n",
            "Cell \u001b[0;32mIn[4], line 56\u001b[0m, in \u001b[0;36mBoosted_NN.loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Get predictions from all learners, then weight them\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators])\n\u001b[0;32m---> 56\u001b[0m     weighted_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# L2 loss\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((Y \u001b[38;5;241m-\u001b[39m weighted_predictions) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (50,) and (50,1,3918) not aligned: 50 (dim 0) != 1 (dim 1)"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "\n",
        "\n",
        "def test_models(dataset, test_size=0.2):\n",
        "    '''\n",
        "        Tests OneLayerNN, Boost on a given dataset.\n",
        "        :param dataset The path to the dataset\n",
        "        :return None\n",
        "    '''\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(dataset):\n",
        "        print('The file {} does not exist'.format(dataset))\n",
        "        exit()\n",
        "\n",
        "    # Load in the dataset\n",
        "    data = np.loadtxt(dataset, skiprows = 1)\n",
        "    X, Y = data[:, 1:], data[:, 0]\n",
        "\n",
        "    # Normalize the features\n",
        "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
        "    Y = Y / 10\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "    print('Running models on {} dataset'.format(dataset))\n",
        "\n",
        "    # Add a bias\n",
        "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
        "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
        "\n",
        "    #### 1-Layer NN ######\n",
        "    print('----- 1-Layer NN -----')\n",
        "    nnmodel = OneLayerNN()\n",
        "    nnmodel.train(X_train_b, Y_train, np.ones(X_train_b.shape[0]), print_loss=False)\n",
        "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0])))\n",
        "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0])))\n",
        "\n",
        "    #### Boosted Neural Networks ######\n",
        "    print('----- Boosted Neural Network -----')\n",
        "    model = Boosted_NN()\n",
        "\n",
        "    model.train(X_train_b, Y_train)\n",
        "\n",
        "    print(X_train_b.shape)\n",
        "\n",
        "\n",
        "    print('Average Training Loss:', model.loss(X_train_b, Y_train))\n",
        "    print('Average Testing Loss:', model.loss(X_test_b, Y_test))\n",
        "\n",
        "test_models('wine.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h3Us2FgOMgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y7jqYzqTDwBc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data2060",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

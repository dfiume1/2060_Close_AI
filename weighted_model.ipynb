{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIZqWCd2L6C"
      },
      "source": [
        "# Boosting With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJqYh6u2Qtz"
      },
      "source": [
        "## Markdown Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Representation\n",
        "\n",
        "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
        "\n",
        "$$E(H, T) = {x \\to sign(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
        "\n",
        "### Loss\n",
        "\n",
        "We are using L2 loss for both the one-layer neural network and the boosted ensemble of hypotheses.\n",
        "For each single-layer neural network, loss is defined as:\n",
        "$$L_S(h_{\\bf t}) = \\sum\\limits_{i=1}^m(y_{i}-h_{\\bf t}({\\bf x}_{i}))^{2}$$\n",
        "\n",
        "where *y*<sub>i</sub> is the target value of *i*<sup>th</sup>\n",
        "sample and $h_{\\bf t}({\\bf x})$ is the predicted value of that\n",
        "sample given the learned model weights.\n",
        "\n",
        "For the ensemble, loss is then defined as:\n",
        "$$L_S(E(H, T)) = \\sum\\limits_{i=1}^m(y_{i}-E(H, T)({\\bf x}_{i}))^{2}$$\n",
        "\n",
        "### Optimzer\n",
        "\n",
        "Input:\n",
        "Training set $S = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_m, y_m)\\}$\n",
        "Weak learner $Wl$\n",
        "Number of rounds $T$\n",
        "\n",
        "Initialize:\n",
        "$D^{(1)} = \\left(\\frac{1}{m}, \\ldots, \\frac{1}{m}\\right)$\n",
        "\n",
        "For $t = 1, \\ldots, T$:\n",
        "\n",
        "$h_t = WL(D^{(t)}, S)$\n",
        "\n",
        "$\\epsilon_t = \\sum_{i=1}^m D_i^{(t)} \\mathbb{1}[y_i \\neq h_t(\\mathbf{x}_i)]$\n",
        "\n",
        "$w_t = \\frac{1}{2} \\log \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$\n",
        "\n",
        "$D_i^{(t+1)} = \\frac{D_i^{(t)} \\exp(-w_t y_i h_t(\\mathbf{x}_i))}{\\sum_{j=1}^m D_j^{(t)} \\exp(-w_t y_j h_t(\\mathbf{x}_j))} \\quad \\forall i = 1, \\ldots, m$\n",
        "\n",
        "Output:\n",
        "The hypothesis: $h_S(\\mathbf{x}) = \\text{sign}\\left(\\sum_{t=1}^T w_t h_t(\\mathbf{x})\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppdKF2TvGOLo"
      },
      "source": [
        "### Representation\n",
        "\n",
        "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
        "\n",
        "$$E(H, T) = {x \\to sign(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
        "\n",
        "### Loss\n",
        "\n",
        "$ε_t = L_D(t)(h_t) = \\sum_{i=0}^mD_i^t * \\ell(h_t(x_i))$, where $D(t) ∈ R^m$\n",
        "\n",
        "In our model, the loss function $\\ell$ can either be 0-1 loss or l2 loss, depending on the type of dataset we are dealing with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUF8LgSz3Nna"
      },
      "source": [
        "### Check Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKoyt0u2tiR",
        "outputId": "357f42eb-bd03-4dcb-a75b-cd289e2a5cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.5\n",
            "\n",
            "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from packaging.version import parse as Version\n",
        "from platform import python_version\n",
        "\n",
        "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
        "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "except ImportError:\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % sys.version)\n",
        "\n",
        "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
        "    mod = None\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        if pkg in {'PIL'}:\n",
        "            ver = mod.VERSION\n",
        "        else:\n",
        "            ver = mod.__version__\n",
        "        if Version(ver) == Version(min_ver):\n",
        "            print(OK, \"%s version %s is installed.\"\n",
        "                  % (lib, min_ver))\n",
        "        else:\n",
        "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
        "                  % (lib, min_ver, ver))\n",
        "    except ImportError:\n",
        "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
        "    return mod\n",
        "\n",
        "\n",
        "# first check the python version\n",
        "pyversion = Version(python_version())\n",
        "\n",
        "if pyversion >= Version(\"3.12.5\"):\n",
        "    print(OK, \"Python version is %s\" % pyversion)\n",
        "elif pyversion < Version(\"3.12.5\"):\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % pyversion)\n",
        "else:\n",
        "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
        "\n",
        "\n",
        "print()\n",
        "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\",\n",
        "                'pandas': \"2.2.2\"}\n",
        "\n",
        "# now the dependencies\n",
        "for lib, required_version in list(requirements.items()):\n",
        "    import_version(lib, required_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfCnqsOY3WLp"
      },
      "source": [
        "## Model Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npX8gMU3Y0I"
      },
      "source": [
        "### Weak Learner: One Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "dFNBypLJ1nJY"
      },
      "outputs": [],
      "source": [
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# class DecisionTreeStump:\n",
        "#     def __init__(self):\n",
        "#         self.tree = DecisionTreeRegressor(max_depth=1)  # depth 1 for stump\n",
        "\n",
        "#     def train(self, X, Y, data_weights):\n",
        "#         self.tree.fit(X, Y, sample_weight=data_weights)\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         return self.tree.predict(X)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def l2_loss_weight(predictions,Y,weights):\n",
        "    '''\n",
        "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
        "        that are weighted\n",
        "        :param Y: A 1D Numpy array with real values (float64)\n",
        "        :param predictions: A 1D Numpy array of the same size of Y\n",
        "        :param weights: A 1D Numpy array of the same size of Y,\n",
        "        :return: Weighted L2 loss using predictions for Y.\n",
        "    '''\n",
        "\n",
        "    return np.sum(weights * (predictions - Y)**2)\n",
        "\n",
        "class OneLayerNN:\n",
        "    '''\n",
        "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        @attrs:\n",
        "            weights: The weights of the neural network model.\n",
        "            batch_size: The number of examples in each batch\n",
        "            learning_rate: The learning rate to use for SGD\n",
        "            epochs: The number of times to pass through the dataset\n",
        "            v: The resulting predictions computed during the forward pass\n",
        "        '''\n",
        "        # initialize self.weights in train()\n",
        "        self.weights = None\n",
        "        self.learning_rate = 0.001\n",
        "        self.epochs = 25\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # initialize self.v in forward_pass()\n",
        "        self.v = None\n",
        "    \n",
        "    def train(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Trains the OneLayerNN model using SGD.\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :param print_loss: If True, print the loss after each epoch.\n",
        "        :return: None\n",
        "        '''\n",
        "        # TODO: initialize weights\n",
        "        num_examples, num_features = X.shape\n",
        "        self.weights = np.random.uniform(0, 1, (1, num_features))\n",
        "        self.data_weights = data_weights\n",
        "        # TODO: Train network for certain number of epochs\n",
        "        for epoch in range(self.epochs):\n",
        "            # TODO: Shuffle the examples (X) and labels (Y)\n",
        "            indices = np.random.permutation(num_examples)\n",
        "            X_shuffled = X[indices]\n",
        "            Y_shuffled = Y[indices]\n",
        "        # TODO: We need to iterate over each data point for each epoch\n",
        "        # iterate through the examples in batch size increments\n",
        "            for i in range(num_examples):\n",
        "\n",
        "                x_i = X_shuffled[i].reshape(1, num_features)\n",
        "                y_i = Y_shuffled[i].reshape(1, 1)            \n",
        "\n",
        "                # TODO: Perform the forward and backward pass on the current batch\n",
        "                self.forward_pass(x_i)\n",
        "                self.backward_pass(x_i, y_i)\n",
        "\n",
        "            # Print the loss after every epoch\n",
        "            # if print_loss:\n",
        "            #     print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        '''\n",
        "        Computes the predictions for a single layer given examples X and\n",
        "        stores them in self.v\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :return: None\n",
        "        '''\n",
        "        # TODO:\n",
        "        self.v = np.dot(self.weights, X.T)\n",
        "\n",
        "    def backward_pass(self, X, Y):\n",
        "        '''\n",
        "        Computes the weights gradient and updates self.weights\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: None\n",
        "        '''\n",
        "        # TODO: Compute the gradients for the model's weights using backprop\n",
        "        gradient = self.backprop(X, Y)\n",
        "        # TODO: Update the weights using gradient descent\n",
        "        self.gradient_descent(gradient)\n",
        "\n",
        "    def backprop(self, X, Y):\n",
        "        '''\n",
        "        Returns the average weights gradient for the given batch\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A 1D Numpy array representing the weights gradient\n",
        "        '''\n",
        "        # TODO: Compute the average weights gradient\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "        loss = self.v - Y\n",
        "        return np.dot(2*loss, X)\n",
        "\n",
        "    def gradient_descent(self, grad_W):\n",
        "        '''\n",
        "        Updates the weights using the given gradient\n",
        "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
        "        :return: None\n",
        "        '''\n",
        "        self.weights -= (self.learning_rate * grad_W)\n",
        "\n",
        "    def loss(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the total squared error on some dataset (X, Y).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the squared error of the model on the dataset\n",
        "        '''\n",
        "        # Perform the forward pass and compute the l2 loss\n",
        "        self.forward_pass(X)\n",
        "        return l2_loss_weight(self.v, Y, data_weights)\n",
        "\n",
        "    def average_loss(self, X, Y, data_weights):\n",
        "        '''\n",
        "        Returns the mean squared error on some dataset (X, Y).\n",
        "        MSE = Total squared error/# of examples\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the mean squared error of the model on the dataset\n",
        "        '''\n",
        "        return self.loss(X, Y, data_weights) / X.shape[0]\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "            Returns the predicted values for some dataset (X).\n",
        "            :param X: 2D Numpy array where each row contains an example\n",
        "            :return: 1D Numpy array containing the predicted values for each example\n",
        "        '''\n",
        "        self.forward_pass(X)\n",
        "        return self.v\n",
        "    \n",
        "# class OneLayerNN:\n",
        "#     '''\n",
        "#         One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
        "#     '''\n",
        "#     def __init__(self):\n",
        "#         '''\n",
        "#         @attrs:\n",
        "#             weights: The weights of the neural network model.\n",
        "#             batch_size: The number of examples in each batch\n",
        "#             learning_rate: The learning rate to use for SGD\n",
        "#             epochs: The number of times to pass through the dataset\n",
        "#             v: The resulting predictions computed during the forward pass\n",
        "#         '''\n",
        "#         # initialize self.weights in train()\n",
        "#         self.weights = None\n",
        "#         self.learning_rate = 0.001\n",
        "#         self.epochs = 25\n",
        "#         self.batch_size = 1\n",
        "\n",
        "#         # initialize self.v in forward_pass()\n",
        "#         self.v = None\n",
        "\n",
        "#     def train(self, X, Y, data_weights, print_loss=False):\n",
        "#         '''\n",
        "#         Trains the OneLayerNN model using SGD.\n",
        "#         :param X: 2D Numpy array where each row contains an example\n",
        "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "#         :param print_loss: If True, print the loss after each epoch.\n",
        "#         :return: None\n",
        "#         '''\n",
        "#         # Initialize weights\n",
        "#         input_size = X.shape[1]\n",
        "#         self.weights = np.random.uniform(0,1,(1, input_size))\n",
        "#         #print(\"Weights Initial Shape: \", self.weights.shape)\n",
        "\n",
        "#         # Train network for certain number of epochs\n",
        "#         for epoch in range(self.epochs):\n",
        "\n",
        "#             # Shuffle the examples (X) and labels (Y)\n",
        "#             rand_index = np.arange(X.shape[0])\n",
        "#             np.random.shuffle(rand_index)\n",
        "#             X_s = X[rand_index]\n",
        "#             Y_s = Y[rand_index]\n",
        "#             data_weights_s = data_weights[rand_index]\n",
        "\n",
        "#              # iterate through the examples in batch size increments\n",
        "#             for i in range((int(np.ceil(X_s.shape[0] / self.batch_size)))):\n",
        "#                 X_batch = X_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "#                 Y_batch = Y_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "#                 data_weights_batch = data_weights_s[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "\n",
        "\n",
        "#                 #Perform the forward and backward pass on the current batch\n",
        "#                 self.forward_pass(X_batch)\n",
        "#                 self.backward_pass(X_batch, Y_batch, data_weights_batch)\n",
        "\n",
        "#             # Print the loss after every epoch\n",
        "#             if print_loss:\n",
        "#                 print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y, data_weights_batch)))\n",
        "\n",
        "#     def forward_pass(self, X):\n",
        "#         '''\n",
        "#         Computes the predictions for a single layer given examples X and\n",
        "#         stores them in self.v\n",
        "#         :param X: 2D Numpy array where each row contains an example.\n",
        "#         :return: None\n",
        "#         '''\n",
        "\n",
        "#         self.v = np.dot(self.weights, X.T) # + bias (no bias for now?)\n",
        "#         #print(\"v shape: \", self.v.shape)\n",
        "\n",
        "\n",
        "\n",
        "#     def backward_pass(self, X, Y, data_weights):\n",
        "#         '''\n",
        "#         Computes the weights gradient and updates self.weights\n",
        "#         :param X: 2D Numpy array where each row contains an example\n",
        "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "#         :return: None\n",
        "#         '''\n",
        "#         # Compute the gradients for the model's weights using backprop\n",
        "#         grads = self.backprop(X,Y, data_weights)\n",
        "\n",
        "#         # Update the weights using gradient descent\n",
        "#         self.gradient_descent(grads)\n",
        "\n",
        "\n",
        "\n",
        "#     def backprop(self, X, Y, data_weights):\n",
        "#         '''\n",
        "#         Returns the average weights gradient for the given batch\n",
        "#         :param X: 2D Numpy array where each row contains an example.\n",
        "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "#         :return: A 1D Numpy array representing the weights gradient\n",
        "#         '''\n",
        "#         # Compute the average weights gradient\n",
        "#         # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "\n",
        "#         # The gradient dL/dw = -2 * xi * predictons (sum(yi-h(xi)))\n",
        "#         m = X.shape[0]\n",
        "#         grad_W = (2/m) * np.dot(X.T, data_weights * (self.v - Y))\n",
        "\n",
        "#         #print(\"Shape of grad_W: \", grad_W.T.shape)\n",
        "\n",
        "#         return grad_W.T\n",
        "\n",
        "#     def gradient_descent(self, grad_W):\n",
        "#         '''\n",
        "#         Updates the weights using the given gradient\n",
        "#         :param grad_W: A 1D Numpy array representing the weights gradient\n",
        "#         :return: None\n",
        "#         '''\n",
        "#         # Update the weights using the given gradient and the learning rate\n",
        "#         # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "#         self.weights = self.weights - (self.learning_rate * grad_W)\n",
        "#         #print(self.weights)\n",
        "\n",
        "\n",
        "#     def loss(self, X, Y, data_weights):\n",
        "#         '''\n",
        "#         Returns the total squared error on some dataset (X, Y).\n",
        "#         :param X: 2D Numpy array where each row contains an example\n",
        "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "#         :return: A float which is the squared error of the model on the dataset\n",
        "#         '''\n",
        "#         # Perform the forward pass and compute the l2 loss\n",
        "#         self.forward_pass(X)\n",
        "#         return l2_loss_weight(self.v, Y, data_weights)\n",
        "\n",
        "#     def average_loss(self, X, Y, data_weights):\n",
        "#         '''\n",
        "#         Returns the mean squared error on some dataset (X, Y).\n",
        "#         MSE = Total squared error/# of examples\n",
        "#         :param X: 2D Numpy array where each row contains an example\n",
        "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "#         :return: A float which is the mean squared error of the model on the dataset\n",
        "#         '''\n",
        "#         return self.loss(X, Y, data_weights) / X.shape[0]\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         '''\n",
        "#         Returns the predicted values for some dataset (X).\n",
        "#         :param X: 2D Numpy array where each row contains an example\n",
        "#         :return: 1D Numpy array containing the predicted values for each example\n",
        "#         '''\n",
        "#         self.forward_pass(X)\n",
        "#         return self.v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3JqPeK3hVC"
      },
      "source": [
        "### Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "eDK23Q7F3gZJ"
      },
      "outputs": [],
      "source": [
        "class Boosted_NN:\n",
        "  def __init__(self, n_estimators=50, learning_rate=0.01, random_state=1):\n",
        "    self.n_estimators = n_estimators\n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "    self.estimator_weights = np.zeros(self.n_estimators)\n",
        "    self.data_weights = []\n",
        "\n",
        "    # Initialize the estimators\n",
        "    self.estimators = []\n",
        "    for i in range(self.n_estimators):\n",
        "      self.estimators.append(OneLayerNN())\n",
        "\n",
        "  def train(self, X, y):\n",
        "    '''\n",
        "    Trains/Fits the Boosting Model using AdaBoost.\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "    '''\n",
        "    # Initialize the data and estimator weights\n",
        "    num_inputs = X.shape[0]\n",
        "    self.data_weights = np.ones(num_inputs) / num_inputs\n",
        "\n",
        "    # For each round/weak learner\n",
        "    for i in range(self.n_estimators):\n",
        "\n",
        "      # Use the weak learner\n",
        "      weak_learner = self.estimators[i]\n",
        "\n",
        "      # Fit the weak learner\n",
        "      weak_learner.train(X, y, self.data_weights)\n",
        "      print(self.data_weights)\n",
        "      #print(\"Before Reshape\", weak_learner.predict(X).shape)\n",
        "      y_pred = weak_learner.predict(X).reshape(num_inputs)\n",
        "      #print(\"y_pred\", y_pred)\n",
        "\n",
        "      e_t = np.sum(self.data_weights * ((y-y_pred) **2)) / np.sum(self.data_weights)\n",
        "      e_t = np.clip(e_t, 1e-10, 0.49)\n",
        "      #e_t = e_t / num_inputs\n",
        "      #print(\"weighted error\", e_t)\n",
        "      \n",
        "      w_t = 0.5 * np.log((1 - e_t) / e_t)\n",
        "      #print(\"w_t\", w_t)\n",
        "\n",
        "      self.estimator_weights[i] = w_t\n",
        "      \n",
        "      self.data_weights *= np.exp(-w_t * (y-y_pred)**2)\n",
        "      self.data_weights = np.clip(self.data_weights, a_min=1e-10, a_max=1e2)\n",
        "      self.data_weights /= np.sum(self.data_weights)\n",
        "      #print(f\"Sum of data weights (should be 1): {np.sum(self.data_weights)}\")  \n",
        "    \n",
        "    self.estimator_weights = self.estimator_weights / np.sum(self.estimator_weights)\n",
        "    print(self.estimator_weights)\n",
        "\n",
        "  def loss(self, X, Y):\n",
        "      # Get predictions from all learners, then weight them\n",
        "      #print(\"Shape of Y\", Y.shape)\n",
        "      #print(\"one prediction\", self.estimators[0].predict(X).shape)\n",
        "      predictions = np.array([e.predict(X) for e in self.estimators])\n",
        "      #print(\"prediction shape\", predictions.shape)\n",
        "      predictions = predictions.reshape(self.n_estimators, Y.shape[0])\n",
        "      #print(\"prediction shape\", predictions.shape)\n",
        "      #print(\"Original Pred\", predictions)\n",
        "      #print(\"Estimator Weights\", self.estimator_weights)\n",
        "      weighted_predictions = np.dot(self.estimator_weights, predictions)\n",
        "      #print(\"Weighted\", weighted_predictions)\n",
        "      #print(Y)\n",
        "      # L2 loss\n",
        "      loss = np.mean((Y - weighted_predictions) ** 2)\n",
        "      return loss\n",
        "  \n",
        "  def average_loss(self, X, Y):\n",
        "        '''\n",
        "        Returns the mean squared error on some dataset (X, Y).\n",
        "        MSE = Total squared error/# of examples\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the mean squared error of the model on the dataset\n",
        "        '''\n",
        "        return self.loss(X, Y) / X.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjphpjYOI1F"
      },
      "source": [
        "## Accuracy on Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "-mPAx8l8ONaB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running models on wine.txt dataset\n",
            "----- 1-Layer NN -----\n",
            "Average Training Loss: 0.5659728561586289\n",
            "Average Testing Loss: 0.5981214364101037\n",
            "----- Boosted Neural Network -----\n",
            "[0.00025523 0.00025523 0.00025523 ... 0.00025523 0.00025523 0.00025523]\n",
            "[0.00025672 0.00024487 0.00025758 ... 0.0002575  0.00024608 0.00025787]\n",
            "[0.00025852 0.00023225 0.00025969 ... 0.00025957 0.00023575 0.00026045]\n",
            "[0.00026053 0.00021911 0.0002615  ... 0.00026156 0.00022536 0.00026311]\n",
            "[0.00026266 0.00020696 0.00026343 ... 0.00026366 0.00021404 0.00026567]\n",
            "[0.00026445 0.00019754 0.00026559 ... 0.00026543 0.00020371 0.00026795]\n",
            "[0.00026705 0.00018389 0.00026888 ... 0.0002679  0.00018979 0.00027136]\n",
            "[0.00027324 0.00015578 0.0002725  ... 0.00027447 0.00015794 0.00027845]\n",
            "[0.00028436 0.00011086 0.00028504 ... 0.00028298 0.00011082 0.00029155]\n",
            "[3.01794562e-04 5.51475907e-05 3.02947402e-04 ... 2.96972159e-04\n",
            " 5.65145007e-05 3.12919061e-04]\n",
            "[0.0215457  0.0215457  0.0215457  0.0215457  0.0215457  0.03134158\n",
            " 0.06373079 0.1369539  0.25317111 0.4070741 ]\n",
            "(3918, 12)\n",
            "Average Training Loss: 0.5607316896272586\n",
            "Average Testing Loss: 0.5862933227874219\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "\n",
        "\n",
        "def test_models(dataset, test_size=0.2):\n",
        "    '''\n",
        "        Tests OneLayerNN, Boost on a given dataset.\n",
        "        :param dataset The path to the dataset\n",
        "        :return None\n",
        "    '''\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(dataset):\n",
        "        print('The file {} does not exist'.format(dataset))\n",
        "        exit()\n",
        "\n",
        "    # Load in the dataset\n",
        "    data = np.loadtxt(dataset, skiprows = 1)\n",
        "    X, Y = data[:, 1:], data[:, 0]\n",
        "\n",
        "    # Normalize the features\n",
        "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
        "    Y = Y \n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "    print('Running models on {} dataset'.format(dataset))\n",
        "\n",
        "    # Add a bias\n",
        "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
        "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
        "\n",
        "    #### 1-Layer NN ######\n",
        "    print('----- 1-Layer NN -----')\n",
        "    nnmodel = OneLayerNN()\n",
        "    nnmodel.train(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
        "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0])))\n",
        "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0])))\n",
        "\n",
        "    #### Boosted Neural Networks ######\n",
        "    print('----- Boosted Neural Network -----')\n",
        "    model = Boosted_NN(n_estimators=10)\n",
        "\n",
        "    model.train(X_train_b, Y_train)\n",
        "\n",
        "    print(X_train_b.shape)\n",
        "    \n",
        "    # this line errors \n",
        "    # Y_train = Y_train.reshape(-1,1)\n",
        "    # Y_test = Y_test.reshape(-1,1)\n",
        "\n",
        "    print('Average Training Loss:', model.loss(X_train_b, Y_train))\n",
        "    print('Average Testing Loss:', model.loss(X_test_b, Y_test))\n",
        "\n",
        "    #### Boosted Neural Networks ######\n",
        "    print('----- Boosted Neural Network -----')\n",
        "\n",
        "test_models('wine.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "7h3Us2FgOMgY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.687\n"
          ]
        }
      ],
      "source": [
        "# Test \n",
        "print(34.35 / 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y7jqYzqTDwBc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data2060",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvIZqWCd2L6C"
   },
   "source": [
    "# Boosting With Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVJqYh6u2Qtz"
   },
   "source": [
    "## Markdown Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Boosting Algorithm\n",
    "\n",
    "Boosting is an ensemble learning method. The idea of boosting algorithm is to create a single strong learner by combining the predictions of weak learners. This method was proposed by Freund and Schapire in the paper \"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\" in 1995 and mainly applied to classification tasks. In boosting, each weak learner will adjust its weight according to the results of the previous round of weak learners in continuous iterations. Boosting iteratively builds weak classifiers or regressors and this new learner will pay special attention to the samples that were misclassified or having bigger errors in the previous round, so as to better handle these samples that did not perform well in the subsequent iterations. Thus, models can gradually optimize the performance. In the final prediction stage, the prediction results of these weak learners will be weighted and summed according to certain weights to obtain the final prediction result. Based on boosting, Freund and Schapire also proposed AdaBoosting, the adaptive boosting algorithm, in the above paper. At the same time, Drucker further applied it to regression problems in 1997.\n",
    "Adaboosting improves the adaptive mechanism: Adaboosting dynamically adjusts the sample weights and the weights of weak learners according to the error during the iteration process. This adaptive mechanism enables AdaBoosting to gradually optimize the model performance and improve the prediction accuracy.\n",
    "\n",
    "### Advantages\n",
    "- According to Drucker's 1997 paper \"Improving Regressors using Boosting Techniques\", AdaBoosting can achieve higher prediction performance by iteratively training multiple weak learners and reasonably combining their prediction results according to the weights.\n",
    "- The AdaBoosting algorithm is relatively simple and easy to understand and implement.\n",
    "\n",
    "### Disadvantages\n",
    "- Since AdaBoosting gives higher weights to wrongly predicted samples, noise and outliers may cause the model to perform poorly. .\n",
    "- AdaBoosting needs to train multiple weak learners during the training process, the amount of training calculations may be heavy.\n",
    "- If the weak learner is too complex or the number of iterations is too many, AdaBoosting may have overfitting problems.\n",
    "\n",
    "### Modification\n",
    "In this project, we used Adaboosting to implement regression instead of classification problems. The main changes made are as follows:\n",
    "- We chose a 1 layer NN as a weak learner because NN has strong fitting ability and is suitable as a weak learner for regression tasks.\n",
    "- We used mean squared error as the loss function instead of 0/1 loss.\n",
    "\n",
    "\n",
    "### Representation\n",
    "\n",
    "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
    "\n",
    "$$E(H, T) = {x \\to \\frac{1}{T}(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
    "\n",
    "$h_t(x)$: Prediction result from the t-th weak learner.\n",
    "\n",
    "$w_t$ Weight of the t-th learner, determined based on its performance.\n",
    "\n",
    "T: Total number of weak learners in the ensemble.\n",
    "\n",
    "As can be seen from the above, by combining $w_t$ and $h_t(x)$, the weak learner with higher accuracy has a greater weight in the final prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss function quantifies the error between the model's predicted value and the true value, providing guidance for the optimization process.\n",
    "\n",
    "We are using weighted L2 loss for the one-layer neural network and MSE for the boosted ensemble of hypotheses.\n",
    "For each single-layer neural network, loss is defined as:\n",
    "$$L_S(h_{\\bf t}) = \\sum\\limits_{i=1}^m w_m *(y_{i}-h_{\\bf t}({\\bf x}_{i}))^{2}$$\n",
    "\n",
    "where *y*<sub>i</sub> is the target value of *i*<sup>th</sup>\n",
    "sample and $h_{\\bf t}({\\bf x})$ is the predicted value of that\n",
    "sample given the learned model weights, and $w_m$ is the weight for the mth data point.\n",
    "\n",
    "For the ensemble, loss is then defined as:\n",
    "$$L_S(E(H, T)) = \\frac{1}{m}\\sum\\limits_{i=1}^m(y_{i}-E(H, T)({\\bf x}_{i}))^{2}$$\n",
    "\n",
    "### Optimzer\n",
    "\n",
    "The iterative optimization process of AdaBoosting mainly consists of five steps:\n",
    "\n",
    "- First, initialization is performed to assign equal weights to all training samples, where $m$ is the number of samples, $D$ is the weight distribution of each training sample, and $D^{(1)} = \\frac{1}{m}$.\n",
    "\n",
    "- Next, in each iteration, a weak learner $h_t$ is trained using the weighted dataset.\n",
    "\n",
    "- The third step is to calculate the error rate $\\epsilon_t$ of the weak learner.\n",
    "\n",
    "- The fourth step is to calculate the weight $w_t$ of the weak learner and adjust the sample weights $D_i^{(t+1)}$ to pay more attention to the misclassified samples.\n",
    "\n",
    "- Finally, we combine the predictions of all weak learners into the final integrated model.\n",
    "\n",
    "Through the flexible weight adjustment and weighted integration strategy of the prediction results during the iteration process, we can gradually improve the accuracy of the results.\n",
    "\n",
    "below is the pesudoCode:\n",
    "\n",
    "Input:\n",
    "- Training set $S = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_m, y_m)\\}$\n",
    "- Weak learner $Wl$\n",
    "- Number of estimators $T$\n",
    "\n",
    "Initialize:\n",
    "$D^{(1)} = \\left(\\frac{1}{m}, \\ldots, \\frac{1}{m}\\right)$\n",
    "\n",
    "For $t = 1, \\ldots, T$:\n",
    "\n",
    "$h_t = WL(D^{(t)}, S)$\n",
    "\n",
    "$\\epsilon_t = \\sum_{i=1}^m D_i^{(t)} abs(y_i - h_t(\\mathbf{x_i}))$\n",
    "\n",
    "$w_t = \\frac{1}{2} \\log \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$\n",
    "\n",
    "$D_i^{(t+1)} = \\frac{D_i^{(t)} \\exp(-w_t y_i h_t(\\mathbf{x}_i))}{\\sum_{j=1}^m D_j^{(t)} \\exp(-w_t y_j h_t(\\mathbf{x}_j))} \\quad \\forall i = 1, \\ldots, m$\n",
    "\n",
    "Output:\n",
    "- The hypothesis: $h_S(\\mathbf{x}) = \\sum_{t=1}^T w_t h_t(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUF8LgSz3Nna"
   },
   "source": [
    "### Check Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHKoyt0u2tiR",
    "outputId": "357f42eb-bd03-4dcb-a75b-cd289e2a5cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.5\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))\n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.5\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.5\"):\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "\n",
    "print()\n",
    "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\",\n",
    "                'pandas': \"2.2.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfCnqsOY3WLp"
   },
   "source": [
    "## Model Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5npX8gMU3Y0I"
   },
   "source": [
    "### Weak Learner: One Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dFNBypLJ1nJY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def l2_loss_weight(predictions,Y,weights):\n",
    "    '''\n",
    "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
    "        that are weighted\n",
    "        :param Y: A 1D Numpy array with real values (float64)\n",
    "        :param predictions: A 1D Numpy array of the same size of Y\n",
    "        :param weights: A 1D Numpy array of the same size of Y,\n",
    "        :return: Weighted L2 loss using predictions for Y.\n",
    "    '''\n",
    "\n",
    "    return np.sum(weights * (predictions - Y)**2)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class OneLayerNN(BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
    "    '''\n",
    "    def __init__(self, learning_rate = 0.001, num_epochs = 25, batch_size = 1):\n",
    "        '''\n",
    "        @attrs:\n",
    "            weights: The weights of the neural network model.\n",
    "            batch_size: The number of examples in each batch\n",
    "            learning_rate: The learning rate to use for SGD\n",
    "            epochs: The number of times to pass through the dataset\n",
    "            v: The resulting predictions computed during the forward pass\n",
    "        '''\n",
    "        # initialize self.weights in fit()\n",
    "        self.weights = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # initialize self.v in forward_pass()\n",
    "        self.v = None\n",
    "        self.data_weights = None\n",
    "    \n",
    "    def fit(self, X, Y, data_weights=None):\n",
    "        '''\n",
    "        Trains the OneLayerNN model using SGD.\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :param print_loss: If True, print the loss after each epoch.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: initialize weights\n",
    "        num_examples, num_features = X.shape\n",
    "        self.weights = np.random.uniform(0, 1, (1, num_features))\n",
    "        if data_weights is None:\n",
    "            self.data_weights = np.ones(num_examples)\n",
    "        else:\n",
    "            self.data_weights = data_weights / max(data_weights)\n",
    "        # TODO: Train network for certain number of epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # TODO: Shuffle the examples (X) and labels (Y)\n",
    "            indices = np.random.permutation(num_examples)\n",
    "            X_shuffled = X[indices]\n",
    "            Y_shuffled = Y[indices]\n",
    "            data_weights_shuffled = self.data_weights[indices]\n",
    "        # TODO: We need to iterate over each data point for each epoch\n",
    "        # iterate through the examples in batch size increments\n",
    "            for i in range(num_examples):\n",
    "\n",
    "                x_i = X_shuffled[i].reshape(1, num_features)\n",
    "                y_i = Y_shuffled[i].reshape(1, 1)            \n",
    "                data_i = data_weights_shuffled[i]\n",
    "                # TODO: Perform the forward and backward pass on the current batch\n",
    "                self.forward_pass(x_i)\n",
    "                self.backward_pass(x_i, y_i, data_i)\n",
    "\n",
    "            # Print the loss after every epoch\n",
    "            # if print_loss:\n",
    "            #     print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        '''\n",
    "        Computes the predictions for a single layer given examples X and\n",
    "        stores them in self.v\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO:\n",
    "        self.v = np.dot(self.weights, X.T).flatten()\n",
    "\n",
    "    def backward_pass(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Computes the weights gradient and updates self.weights\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Compute the gradients for the model's weights using backprop\n",
    "        gradient = self.backprop(X, Y, data_weights)\n",
    "        # TODO: Update the weights using gradient descent\n",
    "        self.gradient_descent(gradient)\n",
    "\n",
    "    def backprop(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Returns the average weights gradient for the given batch\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A 1D Numpy array representing the weights gradient\n",
    "        '''\n",
    "        # TODO: Compute the average weights gradient\n",
    "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
    "        loss = self.v - Y\n",
    "        return np.dot(2*loss*data_weights, X)\n",
    "\n",
    "    def gradient_descent(self, grad_W):\n",
    "        '''\n",
    "        Updates the weights using the given gradient\n",
    "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
    "        :return: None\n",
    "        '''\n",
    "        self.weights -= (self.learning_rate * grad_W)\n",
    "\n",
    "    def loss(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        # Perform the forward pass and compute the l2 loss\n",
    "        self.forward_pass(X)\n",
    "        return l2_loss_weight(self.v, Y, data_weights)\n",
    "\n",
    "    def average_loss(self, X, Y, data_weights):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y, data_weights) / X.shape[0]\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "            Returns the predicted values for some dataset (X).\n",
    "            :param X: 2D Numpy array where each row contains an example\n",
    "            :return: 1D Numpy array containing the predicted values for each example\n",
    "        '''\n",
    "        self.forward_pass(X)\n",
    "        return self.v\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# def l2_loss_weight(predictions,Y,weights):\n",
    "#     '''\n",
    "#         Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
    "#         that are weighted\n",
    "#         :param Y: A 1D Numpy array with real values (float64)\n",
    "#         :param predictions: A 1D Numpy array of the same size of Y\n",
    "#         :param weights: A 1D Numpy array of the same size of Y,\n",
    "#         :return: Weighted L2 loss using predictions for Y.\n",
    "#     '''\n",
    "\n",
    "#     return np.sum(weights * (predictions - Y)**2)\n",
    "\n",
    "#from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# class OneLayerNN(BaseEstimator, RegressorMixin):\n",
    "#     '''\n",
    "#         One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
    "#     '''\n",
    "#     def __init__(self, learning_rate = 0.001, num_epochs = 25, batch_size = 1):\n",
    "#         '''\n",
    "#         @attrs:\n",
    "#             weights: The weights of the neural network model.\n",
    "#             batch_size: The number of examples in each batch\n",
    "#             learning_rate: The learning rate to use for SGD\n",
    "#             epochs: The number of times to pass through the dataset\n",
    "#             v: The resulting predictions computed during the forward pass\n",
    "#         '''\n",
    "#         # initialize self.weights in fit()\n",
    "#         self.weights = None\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = num_epochs\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#         # initialize self.v in forward_pass()\n",
    "#         self.v = None\n",
    "#         self.data_weights = None\n",
    "    \n",
    "#     def fit(self, X, Y, data_weights=None):\n",
    "#         '''\n",
    "#         Trains the OneLayerNN model using SGD.\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :param print_loss: If True, print the loss after each epoch.\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         # TODO: initialize weights\n",
    "#         num_examples, num_features = X.shape\n",
    "#         self.weights = np.random.uniform(0, 1, (1, num_features))\n",
    "#         if data_weights is None:\n",
    "#             data_weights = np.ones(num_examples)\n",
    "#         else:\n",
    "#             self.data_weights = data_weights\n",
    "#         # TODO: Train network for certain number of epochs\n",
    "#         for epoch in range(self.epochs):\n",
    "#             # TODO: Shuffle the examples (X) and labels (Y)\n",
    "#             indices = np.random.permutation(num_examples)\n",
    "#             X_shuffled = X[indices]\n",
    "#             Y_shuffled = Y[indices]\n",
    "#         # TODO: We need to iterate over each data point for each epoch\n",
    "#         # iterate through the examples in batch size increments\n",
    "#             for i in range(num_examples):\n",
    "\n",
    "#                 x_i = X_shuffled[i].reshape(1, num_features)\n",
    "#                 y_i = Y_shuffled[i].reshape(1, 1)            \n",
    "\n",
    "#                 # TODO: Perform the forward and backward pass on the current batch\n",
    "#                 self.forward_pass(x_i)\n",
    "#                 self.backward_pass(x_i, y_i)\n",
    "\n",
    "#             # Print the loss after every epoch\n",
    "#             # if print_loss:\n",
    "#             #     print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "#     def forward_pass(self, X):\n",
    "#         '''\n",
    "#         Computes the predictions for a single layer given examples X and\n",
    "#         stores them in self.v\n",
    "#         :param X: 2D Numpy array where each row contains an example.\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         # TODO:\n",
    "#         self.v = np.dot(self.weights, X.T).flatten()\n",
    "\n",
    "#     def backward_pass(self, X, Y):\n",
    "#         '''\n",
    "#         Computes the weights gradient and updates self.weights\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         # TODO: Compute the gradients for the model's weights using backprop\n",
    "#         gradient = self.backprop(X, Y)\n",
    "#         # TODO: Update the weights using gradient descent\n",
    "#         self.gradient_descent(gradient)\n",
    "\n",
    "#     def backprop(self, X, Y):\n",
    "#         '''\n",
    "#         Returns the average weights gradient for the given batch\n",
    "#         :param X: 2D Numpy array where each row contains an example.\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: A 1D Numpy array representing the weights gradient\n",
    "#         '''\n",
    "#         # TODO: Compute the average weights gradient\n",
    "#         # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
    "#         loss = self.v - Y\n",
    "#         return np.dot(2*loss, X)\n",
    "\n",
    "#     def gradient_descent(self, grad_W):\n",
    "#         '''\n",
    "#         Updates the weights using the given gradient\n",
    "#         :param grad_W: A 1D Numpy array representing the weights gradient\n",
    "#         :return: None\n",
    "#         '''\n",
    "#         self.weights -= (self.learning_rate * grad_W)\n",
    "\n",
    "#     def loss(self, X, Y, data_weights):\n",
    "#         '''\n",
    "#         Returns the total squared error on some dataset (X, Y).\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: A float which is the squared error of the model on the dataset\n",
    "#         '''\n",
    "#         # Perform the forward pass and compute the l2 loss\n",
    "#         self.forward_pass(X)\n",
    "#         return l2_loss_weight(self.v, Y, data_weights)\n",
    "\n",
    "#     def average_loss(self, X, Y, data_weights):\n",
    "#         '''\n",
    "#         Returns the mean squared error on some dataset (X, Y).\n",
    "#         MSE = Total squared error/# of examples\n",
    "#         :param X: 2D Numpy array where each row contains an example\n",
    "#         :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "#         :return: A float which is the mean squared error of the model on the dataset\n",
    "#         '''\n",
    "#         return self.loss(X, Y, data_weights) / X.shape[0]\n",
    "#     def predict(self, X):\n",
    "#         '''\n",
    "#             Returns the predicted values for some dataset (X).\n",
    "#             :param X: 2D Numpy array where each row contains an example\n",
    "#             :return: 1D Numpy array containing the predicted values for each example\n",
    "#         '''\n",
    "#         self.forward_pass(X)\n",
    "#         return self.v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW3JqPeK3hVC"
   },
   "source": [
    "### Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eDK23Q7F3gZJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "class Boosted_Model:\n",
    "  def __init__(self, n_estimators=50, learning_rate=0.5, random_state=1):\n",
    "    self.n_estimators = n_estimators\n",
    "    self.learning_rate = learning_rate\n",
    "    self.random_state = random_state\n",
    "    self.estimator_weights = np.zeros(self.n_estimators)\n",
    "    self.data_weights = []\n",
    "\n",
    "    # Initialize the estimators\n",
    "    self.estimators = []\n",
    "    for i in range(self.n_estimators):\n",
    "      self.estimators.append(OneLayerNN())\n",
    "      \n",
    "\n",
    "  def train(self, X, y):\n",
    "    '''\n",
    "    Trains/Fits the Boosting Model using AdaBoost.\n",
    "    :param X: 2D Numpy array where each row contains an example\n",
    "    :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "    '''\n",
    "    # Initialize the data and estimator weights\n",
    "    num_inputs = X.shape[0]\n",
    "    self.data_weights = np.ones(num_inputs) / num_inputs\n",
    "\n",
    "    # For each round/weak learner\n",
    "    for i in range(self.n_estimators):\n",
    "\n",
    "      # Use the weak learner\n",
    "      weak_learner = self.estimators[i]\n",
    "\n",
    "      # Fit the weak learner\n",
    "      weak_learner.fit(X, y, self.data_weights)\n",
    "      # print(self.data_weights)\n",
    "      #print(\"Before Reshape\", weak_learner.predict(X).shape)\n",
    "      y_pred = weak_learner.predict(X).reshape(num_inputs)\n",
    "      #print(\"y_pred\", y_pred)\n",
    "\n",
    "      e_t = np.sum(self.data_weights * ((y-y_pred) ** 2)) / np.sum(self.data_weights)\n",
    "      #print(e_t)\n",
    "      e_t = np.clip(e_t, 1e-10, 0.49)\n",
    "      \n",
    "      #e_t = e_t / num_inputs\n",
    "      #print(\"weighted error\", e_t)\n",
    "      \n",
    "      w_t = 0.5 * np.log((1 - e_t) / e_t)\n",
    "      #print(\"w_t\", w_t)\n",
    "\n",
    "      self.estimator_weights[i] = w_t\n",
    "  \n",
    "      self.data_weights *= np.exp(-w_t * ((y-y_pred) ** 2))\n",
    "      self.data_weights = np.clip(self.data_weights, a_min=1e-10, a_max=1e2)\n",
    "      self.data_weights /= np.sum(self.data_weights)\n",
    "      #print(self.data_weights)\n",
    "      #print(f\"Sum of data weights (should be 1): {np.sum(self.data_weights)}\")  \n",
    "    \n",
    "    self.estimator_weights /= np.sum(self.estimator_weights)\n",
    "    #print(self.estimator_weights)\n",
    "\n",
    "\n",
    "  def loss(self, X, Y):\n",
    "      # Get predictions from all learners, then weight them\n",
    "      #print(\"Shape of Y\", Y.shape)\n",
    "      #print(\"one prediction\", self.estimators[0].predict(X).shape)\n",
    "      predictions = np.array([e.predict(X).reshape(-1) for e in self.estimators])\n",
    "      #print(\"prediction shape\", predictions.shape)\n",
    "      #predictions = predictions.reshape(self.n_estimators, Y.shape[0])\n",
    "      #print(\"prediction shape\", predictions.shape)\n",
    "      #print(\"Original Pred\", predictions)\n",
    "      #print(\"Estimator Weights\", self.estimator_weights)\n",
    "      weighted_predictions = np.dot(self.estimator_weights, predictions)\n",
    "      #print(\"Weighted\", weighted_predictions)\n",
    "      #print(Y)\n",
    "      # L2 loss\n",
    "      loss = np.mean((Y - weighted_predictions) ** 2)\n",
    "      return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def test_OneLayerNN():\n",
    "    '''\n",
    "    Tests for OneLayerNN Model Weights Gradient\n",
    "    '''\n",
    "\n",
    "    test_model = OneLayerNN()\n",
    "\n",
    "    # Creates Test Data\n",
    "    x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "    y = np.array([0,0,1,1,0])\n",
    "    no_weights = [1, 1, 1, 1, 1]\n",
    "\n",
    "    # Test Model Train \n",
    "    test_model.fit(x_bias, y, no_weights)\n",
    "\n",
    "    act_weights = test_model.weights\n",
    "    exp_weights = np.array([[ 0.17817953, -0.03543112,  0.34761945]])\n",
    "\n",
    "    print('----Testing 1-Layer NN Gradients-----')\n",
    "\n",
    "    print(\"\\nTesting layer one weights gradient.\")\n",
    "    \n",
    "    # Test layer 1 weights\n",
    "    if not hasattr(act_weights, \"shape\"):\n",
    "        print(\"Layer one weights gradient is not a numpy array. \\n\")\n",
    "    elif act_weights.shape != (1, 3):\n",
    "        print(\n",
    "            f\"Incorrect shape for layer one weights gradient.\\nExpected: {(1, 3)} \\nActual: {act_weights.shape} \\n\")\n",
    "    elif not act_weights == pytest.approx(exp_weights, .01):\n",
    "        print(\n",
    "            f\"Incorrect values for layer one weights gradient.\\nExpected: {exp_weights} \\nActual: {act_weights} \\n\")\n",
    "    else:\n",
    "        print(\"Layer one weights gradient is correct.\\n\")\n",
    "\n",
    "    print('----Testing 1-Layer NN with Data Weights-----')\n",
    "\n",
    "    data_weights_1 = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "    data_weights_2 = [0, 0, 0, 0, 1]\n",
    "\n",
    "    test_model1 = OneLayerNN()\n",
    "    test_model2 = OneLayerNN()\n",
    "    # Test Model Train \n",
    "    test_model1.loss(x_bias, y, data_weights_1)\n",
    "    test_model2.loss(x_bias, y, data_weights_2)\n",
    "\n",
    "    act_weights1 = test_model1.weights\n",
    "    act_weights2 = test_model2.weights\n",
    "    exp_weights1 = np.array([[ 0.17817953, -0.03543112,  0.34761945]])\n",
    "    exp_weights2 = np.array([[ 0.17817953, -0.03543112,  0.34761945]])\n",
    "\n",
    "    if not hasattr(act_weights, \"shape\"):\n",
    "        print(\"Layer one weights gradient is not a numpy array. \\n\")\n",
    "    elif act_weights.shape != (1, 3):\n",
    "        print(\n",
    "            f\"Incorrect shape for layer one weights gradient.\\nExpected: {(1, 3)} \\nActual: {act_weights.shape} \\n\")\n",
    "    elif not act_weights1 == pytest.approx(exp_weights1, .01):\n",
    "        print(\n",
    "            f\"Incorrect values for layer one weights gradient.\\nExpected: {exp_weights} \\nActual: {act_weights} \\n\")\n",
    "    elif not act_weights2 == pytest.approx(exp_weights2, .01):\n",
    "        print(\n",
    "            f\"Incorrect values for layer one weights gradient.\\nExpected: {exp_weights} \\nActual: {act_weights} \\n\")\n",
    "    else:\n",
    "        print(\"Layer one weights gradient with weighted data is correct.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Testing for Boosted_NN Model-----\n",
      "\n",
      "Testing with standard test case.\n",
      "Model computed loss: 0.022635030224767767\n",
      "Manually computed loss: 0.022635030224767767\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Testing with a single data point.\n",
      "Model computed loss for single data point: 1.0\n",
      "Manually computed loss for single data point: 1.0\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Testing with identical input values.\n",
      "Model computed loss for identical inputs: 0.3252697329715408\n",
      "Manually computed loss for single data point: 0.3252697329715408\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Testing with all labels being the same.\n",
      "Model computed loss for same labels: 0.3505038677395824\n",
      "Manually computed loss for same labels: 0.3505038677395824\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Testing with high-dimensional input.\n",
      "Model computed loss for high-dimensional input: 0.34004895190430995\n",
      "Manually computed loss for high-dimensional input: 0.34004895190430995\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Testing with a very large dataset.\n",
      "Model computed loss for very large dataset: 0.2632574489721467\n",
      "Manually computed loss for very large dataset: 0.2632574489721467\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Testing with edge inputs (all zeros and ones).\n",
      "Model computed loss for edge inputs: 0.10650355274806471\n",
      "Manually computed loss for edge inputs: 0.10650355274806471\n",
      "Model computed loss is correct.\n",
      "\n",
      "Testing estimator weights normalization.\n",
      "Estimator weights are normalized correctly.\n",
      "\n",
      "Testing individual estimator predictions.\n",
      "Weak learner predictions shape is correct.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_loss_by_hand(X_test, Y_test, boosted_model):\n",
    "    predictions = np.array([e.predict(X_test).reshape(-1) for e in boosted_model.estimators])\n",
    "    weighted_predictions = np.dot(boosted_model.estimator_weights, predictions)\n",
    "    squared_errors = (Y_test - weighted_predictions) ** 2\n",
    "    total_loss = np.sum(squared_errors)\n",
    "    average_loss = total_loss / len(Y_test)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def test_Boosted_NN():\n",
    "\n",
    "    print('-----Testing for Boosted_NN Model-----')\n",
    "\n",
    "    np.random.seed(4)\n",
    "    random.seed(4)\n",
    "\n",
    "    # Standard Test Case\n",
    "    print(\"\\nTesting with standard test case.\")\n",
    "    boosted_model = Boosted_Model(n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "    X_test = np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]])\n",
    "    Y_test = np.array([0, 0, 1, 1, 0])\n",
    "    boosted_model.train(X_test, Y_test)\n",
    "    computed_loss = boosted_model.loss(X_test, Y_test)\n",
    "    average_loss = calculate_loss_by_hand(X_test, Y_test, boosted_model)\n",
    "    print(f\"Model computed loss: {computed_loss}\")\n",
    "    print(f\"Manually computed loss: {average_loss}\")\n",
    "    if computed_loss == average_loss:\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"odel computed loss is NOT correct.\")\n",
    "\n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test.shape[0])}\\n\"\n",
    "              f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Edge Case 1: Single Data Point\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"\\nTesting with a single data point.\")\n",
    "    X_test_single = np.array([[0, 0, 0]])\n",
    "    Y_test_single = np.array([1])\n",
    "    boosted_model.train(X_test_single, Y_test_single)\n",
    "    computed_loss_single = boosted_model.loss(X_test_single, Y_test_single)\n",
    "    average_loss_single = calculate_loss_by_hand(X_test_single, Y_test_single, boosted_model)\n",
    "    print(f\"Model computed loss for single data point: {computed_loss_single}\")\n",
    "    print(f\"Manually computed loss for single data point: {average_loss_single}\")\n",
    "    if computed_loss_single == average_loss_single:\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"Model computed loss is NOT correct.\")\n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test.shape[0])}\\n\"\n",
    "              f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")\n",
    "        \n",
    "    \n",
    "\n",
    "    # Edge Case 2: All Inputs the Same\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"\\nTesting with identical input values.\")\n",
    "    X_test_identical = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "    Y_test_identical = np.array([0, 0, 1, 1, 0])\n",
    "    boosted_model.train(X_test_identical, Y_test_identical)\n",
    "    computed_loss_identical = boosted_model.loss(X_test_identical, Y_test_identical)\n",
    "    average_loss_identical = calculate_loss_by_hand(X_test_identical, Y_test_identical, boosted_model)\n",
    "    print(f\"Model computed loss for identical inputs: {computed_loss_identical}\")\n",
    "    print(f\"Manually computed loss for single data point: {average_loss_identical}\")\n",
    "    if computed_loss_identical == average_loss_identical:\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"Model computed loss is NOT correct.\")\n",
    "    \n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test.shape[0])}\\n\"\n",
    "              f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")    \n",
    "    \n",
    "\n",
    "    # Edge Case 3: All Labels Same\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"\\nTesting with all labels being the same.\")\n",
    "    X_test_labels_same = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])\n",
    "    Y_test_labels_same = np.array([1, 1, 1, 1, 1])  # All labels are 1\n",
    "    boosted_model.train(X_test_labels_same, Y_test_labels_same)\n",
    "    computed_loss_labels_same = boosted_model.loss(X_test_labels_same, Y_test_labels_same)\n",
    "    average_loss_labels_same = calculate_loss_by_hand(X_test_labels_same, Y_test_labels_same, boosted_model)\n",
    "    print(f\"Model computed loss for same labels: {computed_loss_labels_same}\")\n",
    "    print(f\"Manually computed loss for same labels: {average_loss_labels_same}\")\n",
    "    if computed_loss_labels_same == average_loss_labels_same:\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"Model computed loss is NOT correct.\")\n",
    "        \n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test.shape[0])}\\n\"\n",
    "              f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")  \n",
    "        \n",
    "\n",
    "\n",
    "    # Edge Case 4: High-Dimensional Input\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"\\nTesting with high-dimensional input.\")\n",
    "    X_test_high_dim = np.random.rand(5, 50)  # 5 samples, 50 features\n",
    "    Y_test_high_dim = np.array([0, 1, 0, 1, 0])\n",
    "    boosted_model.train(X_test_high_dim, Y_test_high_dim)\n",
    "    computed_loss_high_dim = boosted_model.loss(X_test_high_dim, Y_test_high_dim)\n",
    "    average_loss_high_dim = calculate_loss_by_hand(X_test_high_dim, Y_test_high_dim, boosted_model)\n",
    "    print(f\"Model computed loss for high-dimensional input: {computed_loss_high_dim}\")\n",
    "    print(f\"Manually computed loss for high-dimensional input: {average_loss_high_dim}\")\n",
    "    if np.isclose(computed_loss_high_dim, average_loss_high_dim, atol=1e-6):\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"Model computed loss is NOT correct.\")\n",
    "\n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test_high_dim).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test_high_dim.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test_high_dim.shape[0])}\\n\"\n",
    "            f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")\n",
    "\n",
    "    # Edge Case 5: Very Large Dataset\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"\\nTesting with a very large dataset.\")\n",
    "    X_test_large = np.random.rand(10000, 10)  # 10,000 samples, 10 features\n",
    "    Y_test_large = np.random.randint(0, 2, size=10000)\n",
    "    boosted_model.train(X_test_large, Y_test_large)\n",
    "    computed_loss_large = boosted_model.loss(X_test_large, Y_test_large)\n",
    "    average_loss_large = calculate_loss_by_hand(X_test_large, Y_test_large, boosted_model)\n",
    "    print(f\"Model computed loss for very large dataset: {computed_loss_large}\")\n",
    "    print(f\"Manually computed loss for very large dataset: {average_loss_large}\")\n",
    "    if np.isclose(computed_loss_large, average_loss_large, atol=1e-6):\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"Model computed loss is NOT correct.\")\n",
    "\n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test_large).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test_large.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test_large.shape[0])}\\n\"\n",
    "            f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")\n",
    "\n",
    "    # Edge Case 6: Edge Inputs (Zeros and Ones)\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"\\nTesting with edge inputs (all zeros and ones).\")\n",
    "    X_test_edges = np.array([[0, 0, 0], [1, 1, 1], [0, 1, 0], [1, 0, 1], [1, 1, 0]])\n",
    "    Y_test_edges = np.array([0, 1, 0, 1, 0])\n",
    "    boosted_model.train(X_test_edges, Y_test_edges)\n",
    "    computed_loss_edges = boosted_model.loss(X_test_edges, Y_test_edges)\n",
    "    average_loss_edges = calculate_loss_by_hand(X_test_edges, Y_test_edges, boosted_model)\n",
    "    print(f\"Model computed loss for edge inputs: {computed_loss_edges}\")\n",
    "    print(f\"Manually computed loss for edge inputs: {average_loss_edges}\")\n",
    "    if np.isclose(computed_loss_edges, average_loss_edges, atol=1e-6):\n",
    "        print(\"Model computed loss is correct.\")\n",
    "    else:\n",
    "        print(\"Model computed loss is NOT correct.\")\n",
    "\n",
    "    # Estimator Weights Normalization\n",
    "    print(\"\\nTesting estimator weights normalization.\")\n",
    "    weight_sum = np.sum(boosted_model.estimator_weights)\n",
    "    if not np.isclose(weight_sum, 1.0, atol=1e-6):\n",
    "        print(f\"Estimator weights are not normalized.\\nSum of weights: {weight_sum}\")\n",
    "    else:\n",
    "        print(\"Estimator weights are normalized correctly.\")\n",
    "\n",
    "    # Individual Estimator Predictions\n",
    "    print(\"\\nTesting individual estimator predictions.\")\n",
    "    weak_predictions = []\n",
    "    for estimator in boosted_model.estimators:\n",
    "        pred = estimator.predict(X_test_edges).reshape(-1)\n",
    "        weak_predictions.append(pred)\n",
    "    weak_predictions = np.array(weak_predictions)\n",
    "\n",
    "    if weak_predictions.shape != (boosted_model.n_estimators, X_test_edges.shape[0]):\n",
    "        print(f\"Incorrect shape for weak learner predictions.\\nExpected: {(boosted_model.n_estimators, X_test_edges.shape[0])}\\n\"\n",
    "            f\"Actual: {weak_predictions.shape}\")\n",
    "    else:\n",
    "        print(\"Weak learner predictions shape is correct.\")\n",
    "\n",
    "test_Boosted_NN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show that it can be reproduced by sklearn's AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- Our Boosted Network (Our NN) -----\n",
      "Average Training Loss: 0.5774640765060357\n",
      "Average Testing Loss: 0.5628175706521719\n",
      "----- sklearn Boosted Network (Our NN) -----\n",
      "Average Training Loss: 0.573942098976232\n",
      "Average Testing Loss: 0.5657701998109839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "\n",
    "def test_sklearn(dataset, test_size=0.2):\n",
    "    '''\n",
    "        Tests OneLayerNN, Boost on a given dataset.\n",
    "        :param dataset The path to the dataset\n",
    "        :return None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print('The file {} does not exist'.format(dataset))\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows = 1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    Y = Y \n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    " \n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    print('----- Our Boosted Network (Our NN) -----')\n",
    "    n_estimators = 30\n",
    "    learning = 1\n",
    "\n",
    "    model = Boosted_Model(n_estimators=n_estimators, learning_rate=learning)\n",
    "    model.train(X_train_b, Y_train)\n",
    "  \n",
    "    print('Average Training Loss:', (model.loss(X_train_b, Y_train)))\n",
    "    print('Average Testing Loss:', (model.loss(X_test_b, Y_test)))\n",
    "\n",
    "    #### sklearn Boosted with our NN ######\n",
    "    print('----- sklearn Boosted Network (Our NN) -----')\n",
    "\n",
    "    model = AdaBoostRegressor(OneLayerNN(),n_estimators=n_estimators, learning_rate=learning)\n",
    "    model.fit(X_train_b, Y_train)\n",
    "  \n",
    "    print('Average Training Loss:', mean_squared_error(model.predict(X_train_b), Y_train))\n",
    "    print('Average Testing Loss:', mean_squared_error(model.predict(X_test_b), Y_test))\n",
    "\n",
    "test_sklearn('wine.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVjphpjYOI1F"
   },
   "source": [
    "## Accuracy on Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-mPAx8l8ONaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- 1-Layer NN -----\n",
      "Average Training Loss: 0.5803284797200633\n",
      "Average Testing Loss: 0.5161149193644858\n",
      "----- Boosted Neural Network -----\n",
      "Average Training Loss: 0.583356861431039\n",
      "Average Testing Loss: 0.5207409769115772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "\n",
    "\n",
    "def test_models(dataset, test_size=0.2):\n",
    "    '''\n",
    "        Tests OneLayerNN, Boost on a given dataset.\n",
    "        :param dataset The path to the dataset\n",
    "        :return None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print('The file {} does not exist'.format(dataset))\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows = 1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    Y = Y \n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.fit(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0])))\n",
    "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0])))\n",
    "\n",
    "    #### Boosted Neural Networks ######\n",
    "    print('----- Boosted Neural Network -----')\n",
    "    model = Boosted_Model(n_estimators=25)\n",
    "\n",
    "    model.train(X_train_b, Y_train)\n",
    "\n",
    "    #print(X_train_b.shape)\n",
    "    \n",
    "    # this line errors \n",
    "    # Y_train = Y_train.reshape(-1,1)\n",
    "    # Y_test = Y_test.reshape(-1,1)\n",
    "\n",
    "    print('Average Training Loss:', model.loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', model.loss(X_test_b, Y_test))\n",
    "\n",
    "test_models('wine.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 1-Layer NN -----\n",
      "----- Boosted Neural Networks -----\n",
      "Testing Boosted_NN with n_estimators=10, learning_rate=0.01\n",
      "Training Loss: 0.5646675320664273, Testing Loss: 0.5732805263639803\n",
      "Testing Boosted_NN with n_estimators=10, learning_rate=0.1\n",
      "Training Loss: 0.5649025065790748, Testing Loss: 0.5700629911574575\n",
      "Testing Boosted_NN with n_estimators=10, learning_rate=0.5\n",
      "Training Loss: 0.5655933045619268, Testing Loss: 0.5692898533658036\n",
      "Testing Boosted_NN with n_estimators=50, learning_rate=0.01\n",
      "Training Loss: 0.5750694653718065, Testing Loss: 0.5919309975247627\n",
      "Testing Boosted_NN with n_estimators=50, learning_rate=0.1\n",
      "Training Loss: 0.5749840074937936, Testing Loss: 0.5912707501286671\n",
      "Testing Boosted_NN with n_estimators=50, learning_rate=0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m     ax\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 87\u001b[0m \u001b[43mtest_models_with_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwine.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m, in \u001b[0;36mtest_models_with_hyperparameters\u001b[0;34m(dataset, test_size, n_estimators_list, learning_rate_list)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting Boosted_NN with n_estimators=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_estimators\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m Boosted_Model(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(X_train_b, Y_train)\n\u001b[1;32m     59\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(X_test_b, Y_test)\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mBoosted_Model.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators[i]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Fit the weak learner\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mweak_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(self.data_weights)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#print(\"Before Reshape\", weak_learner.predict(X).shape)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m weak_learner\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mreshape(num_inputs)\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mOneLayerNN.fit\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# TODO: Perform the forward and backward pass on the current batch\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pass(x_i)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 97\u001b[0m, in \u001b[0;36mOneLayerNN.backward_pass\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mComputes the weights gradient and updates self.weights\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m:param X: 2D Numpy array where each row contains an example\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m:param Y: 1D Numpy array containing the corresponding values for each example\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m:return: None\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# TODO: Compute the gradients for the model's weights using backprop\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# TODO: Update the weights using gradient descent\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_descent(gradient)\n",
      "Cell \u001b[0;32mIn[2], line 111\u001b[0m, in \u001b[0;36mOneLayerNN.backprop\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# TODO: Compute the average weights gradient\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\u001b[39;00m\n\u001b[1;32m    110\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv \u001b[38;5;241m-\u001b[39m Y\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/numpy/_core/multiarray.py:750\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mdot)\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(a, b, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    838\u001b[0m \n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_models_with_hyperparameters(dataset, test_size=0.2, n_estimators_list=[10, 50, 100], learning_rate_list=[0.01, 0.1, 0.5]):\n",
    "    '''\n",
    "        Tests OneLayerNN and Boosted_NN with varying hyperparameters.\n",
    "        :param dataset: The path to the dataset\n",
    "        :param test_size: Fraction of the dataset to be used for testing\n",
    "        :param n_estimators_list: List of values for the number of estimators\n",
    "        :param learning_rate_list: List of values for the learning rate\n",
    "        :return: None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f'The file {dataset} does not exist')\n",
    "        return\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Add a bias term to the features\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    # Results storage for plotting\n",
    "    nn_train_losses = []\n",
    "    nn_test_losses = []\n",
    "    boosted_train_losses = {}\n",
    "    boosted_test_losses = {}\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.fit(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    nn_train_loss = nnmodel.average_loss(X_train_b, Y_train, np.ones(X_train_b.shape[0]))\n",
    "    nn_test_loss = nnmodel.average_loss(X_test_b, Y_test, np.ones(X_test_b.shape[0]))\n",
    "    nn_train_losses.append(nn_train_loss)\n",
    "    nn_test_losses.append(nn_test_loss)\n",
    "    # print('Average Training Loss (1-Layer NN):', nn_train_loss)\n",
    "    # print('Average Testing Loss (1-Layer NN):', nn_test_loss)\n",
    "\n",
    "    #### Boosted Neural Networks ######\n",
    "    print('----- Boosted Neural Networks -----')\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            print(f'Testing Boosted_NN with n_estimators={n_estimators}, learning_rate={learning_rate}')\n",
    "            model = Boosted_Model(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "            model.train(X_train_b, Y_train)\n",
    "            train_loss = model.loss(X_train_b, Y_train)\n",
    "            test_loss = model.loss(X_test_b, Y_test)\n",
    "            print(f'Training Loss: {train_loss}, Testing Loss: {test_loss}')\n",
    "\n",
    "            # Store results\n",
    "            boosted_train_losses[(n_estimators, learning_rate)] = train_loss\n",
    "            boosted_test_losses[(n_estimators, learning_rate)] = test_loss\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot for Boosted NN\n",
    "    for n_estimators in n_estimators_list:\n",
    "        train_losses = [boosted_train_losses[(n_estimators, lr)] for lr in learning_rate_list]\n",
    "        test_losses = [boosted_test_losses[(n_estimators, lr)] for lr in learning_rate_list]\n",
    "        \n",
    "        ax.plot(learning_rate_list, train_losses, marker='o', label=f'Train Loss (n_estimators={n_estimators})')\n",
    "        ax.plot(learning_rate_list, test_losses, marker='x', label=f'Test Loss (n_estimators={n_estimators})', linestyle='--')\n",
    "    \n",
    "    ax.set_title('Boosted NN Loss vs Learning Rate')\n",
    "    ax.set_xlabel('Learning Rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xscale('log')  # Keep the log scale for learning rates\n",
    "    ax.set_xticks(learning_rate_list)  # Explicitly set the ticks\n",
    "    ax.get_xaxis().set_major_formatter(plt.ScalarFormatter())  # Ensure proper formatting of tick labels\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "test_models_with_hyperparameters('wine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing for Boosted_NN Model Correctness-----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeak learner predictions shape is correct.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtest_Boosted_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 13\u001b[0m, in \u001b[0;36mtest_Boosted_NN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     11\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m \u001b[43mboosted_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m computed_loss \u001b[38;5;241m=\u001b[39m boosted_model\u001b[38;5;241m.\u001b[39mloss(X_test, Y_test)\n\u001b[1;32m     16\u001b[0m expected_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n",
      "Cell \u001b[0;32mIn[53], line 35\u001b[0m, in \u001b[0;36mBoosted_Model.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators[i]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Fit the weak learner\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mweak_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(self.data_weights)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#print(\"Before Reshape\", weak_learner.predict(X).shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m weak_learner\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mreshape(num_inputs)\n",
      "Cell \u001b[0;32mIn[52], line 67\u001b[0m, in \u001b[0;36mOneLayerNN.fit\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m     65\u001b[0m num_examples, num_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, num_features))\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     data_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(num_examples)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing for Boosted_NN vs Weak Learner-----\n",
      "\n",
      "---Testing on Synthetic small dataset---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoosted model does not improve in average loss over the weak learner.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mtest_Boosted_vs_WeakLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 38\u001b[0m, in \u001b[0;36mtest_Boosted_vs_WeakLearner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m OneLayerNN()\n\u001b[1;32m     36\u001b[0m data_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(Y_test)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_test)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mboosted_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m weak_learner\u001b[38;5;241m.\u001b[39mfit(X_test, Y_test, data_weights)\n\u001b[1;32m     42\u001b[0m boosted_loss \u001b[38;5;241m=\u001b[39m boosted_model\u001b[38;5;241m.\u001b[39mloss(X_test, Y_test)\n",
      "Cell \u001b[0;32mIn[53], line 35\u001b[0m, in \u001b[0;36mBoosted_Model.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m weak_learner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators[i]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Fit the weak learner\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mweak_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(self.data_weights)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#print(\"Before Reshape\", weak_learner.predict(X).shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m weak_learner\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mreshape(num_inputs)\n",
      "Cell \u001b[0;32mIn[52], line 67\u001b[0m, in \u001b[0;36mOneLayerNN.fit\u001b[0;34m(self, X, Y, data_weights)\u001b[0m\n\u001b[1;32m     65\u001b[0m num_examples, num_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, num_features))\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     data_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(num_examples)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def test_Boosted_vs_WeakLearner():\n",
    "\n",
    "    print('----Testing for Boosted_NN vs Weak Learner-----')\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"X_test\": np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]]),\n",
    "            \"Y_test\": np.array([0, 0, 1, 1, 0]),\n",
    "            \"description\": \"Synthetic small dataset\"\n",
    "        },\n",
    "        {\n",
    "            \"X_test\": np.array([[2, 1, 3], [1, 2, 1], [3, 3, 3], [4, 0, 2], [5, 1, 1]]),\n",
    "            \"Y_test\": np.array([1, 0, 1, 1, 0]),\n",
    "            \"description\": \"Synthetic dataset with varying inputs\"\n",
    "        },\n",
    "        {\n",
    "            \"X_test\": np.array([[0, 0, 1], [1, 1, 1], [2, 2, 1], [3, 3, 1], [4, 4, 1]]),\n",
    "            \"Y_test\": np.array([1, 2, 3, 4, 5]),\n",
    "            \"description\": \"Synthetic regression dataset\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for idx, case in enumerate(test_cases):\n",
    "        X_test = case[\"X_test\"]\n",
    "        Y_test = case[\"Y_test\"]\n",
    "        description = case[\"description\"]\n",
    "\n",
    "        print(f\"\\n---Testing on {description}---\")\n",
    "\n",
    "        np.random.seed(idx+4)\n",
    "        random.seed(idx+4)\n",
    "\n",
    "        boosted_model = Boosted_Model(\"nn\", n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "        weak_learner = OneLayerNN()\n",
    "\n",
    "        data_weights = np.ones(len(Y_test)) / len(Y_test)\n",
    "\n",
    "        boosted_model.train(X_test, Y_test)\n",
    "\n",
    "        weak_learner.fit(X_test, Y_test, data_weights)\n",
    "\n",
    "        boosted_loss = boosted_model.loss(X_test, Y_test)\n",
    "        weak_loss = weak_learner.loss(X_test, Y_test, data_weights)\n",
    "\n",
    "        print(f\"Weak Learner Loss: {weak_loss}\")\n",
    "        print(f\"Boosted Model Loss: {boosted_loss}\")\n",
    "\n",
    "        if boosted_loss < weak_loss:\n",
    "            print(\"Boosted model performs better than the weak learner.\")\n",
    "        else:\n",
    "            print(\"Boosted model does not improve over the weak learner.\")\n",
    "\n",
    "        boosted_avg_loss = boosted_model.loss(X_test, Y_test)\n",
    "        weak_avg_loss = weak_learner.average_loss(X_test, Y_test, data_weights)\n",
    "\n",
    "        print(f\"Weak Learner Average Loss: {weak_avg_loss}\")\n",
    "        print(f\"Boosted Model Average Loss: {boosted_avg_loss}\")\n",
    "\n",
    "        if boosted_avg_loss < weak_avg_loss:\n",
    "            print(\"Boosted model has lower average loss than the weak learner.\")\n",
    "        else:\n",
    "            print(\"Boosted model does not improve in average loss over the weak learner.\")\n",
    "\n",
    "test_Boosted_vs_WeakLearner()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Against the sklearn implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- 1-Layer Stumps -----\n",
      "Average Training Loss: 0.6505357886168747\n",
      "Average Testing Loss: 0.6875904538423815\n",
      "----- Our Boosted Network (Decision Stumps) -----\n",
      "Num Estimators:  100\n",
      "Learning Rate:  1\n",
      "Average Training Loss: 0.7023261831185049\n",
      "Average Testing Loss: 0.719118753024704\n",
      "----- sklearn Boosted Network (Decision Stumps) -----\n",
      "Average Training Loss: 0.5631981193160046\n",
      "Average Testing Loss: 0.6015461722875634\n",
      "----- 1-Layer NN -----\n",
      "Average Training Loss: 0.5614259097798575\n",
      "Average Testing Loss: 0.5985943946824918\n",
      "----- Boosted Neural Network -----\n",
      "Num Estimators:  25\n",
      "Learning Rate:  0.5\n",
      "Average Training Loss: 0.5566066189193396\n",
      "Average Testing Loss: 0.5962706598563772\n",
      "----- sklearn Boosted Network (Our NN) -----\n",
      "Average Training Loss: 0.5659198266971857\n",
      "Average Testing Loss: 0.6042774115349053\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO they should be in the Harvard Citation Format apparently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "y7jqYzqTDwBc"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

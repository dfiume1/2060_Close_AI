{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIZqWCd2L6C"
      },
      "source": [
        "# Boosting With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJqYh6u2Qtz"
      },
      "source": [
        "## Markdown Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppdKF2TvGOLo"
      },
      "source": [
        "### Representation\n",
        "\n",
        "Let $H$ be the class of base, un-boosted hypotheses. Then, $E$ is defined as the ensemble of $H$ weak learners of size $T$.\n",
        "\n",
        "$$E(H, T) = {x \\to sign(Σ(w_t * h_t(x))) : w ∈ R^T, ∀t, h_t ∈ H}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUF8LgSz3Nna"
      },
      "source": [
        "### Check Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKoyt0u2tiR",
        "outputId": "357f42eb-bd03-4dcb-a75b-cd289e2a5cb0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from packaging.version import parse as Version\n",
        "from platform import python_version\n",
        "\n",
        "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
        "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "except ImportError:\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % sys.version)\n",
        "\n",
        "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
        "    mod = None\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        if pkg in {'PIL'}:\n",
        "            ver = mod.VERSION\n",
        "        else:\n",
        "            ver = mod.__version__\n",
        "        if Version(ver) == Version(min_ver):\n",
        "            print(OK, \"%s version %s is installed.\"\n",
        "                  % (lib, min_ver))\n",
        "        else:\n",
        "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
        "                  % (lib, min_ver, ver))\n",
        "    except ImportError:\n",
        "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
        "    return mod\n",
        "\n",
        "\n",
        "# first check the python version\n",
        "pyversion = Version(python_version())\n",
        "\n",
        "if pyversion >= Version(\"3.12.5\"):\n",
        "    print(OK, \"Python version is %s\" % pyversion)\n",
        "elif pyversion < Version(\"3.12.5\"):\n",
        "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
        "                \" but %s is installed.\" % pyversion)\n",
        "else:\n",
        "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
        "\n",
        "\n",
        "print()\n",
        "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\",\n",
        "                'pandas': \"2.2.2\"}\n",
        "\n",
        "# now the dependencies\n",
        "for lib, required_version in list(requirements.items()):\n",
        "    import_version(lib, required_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfCnqsOY3WLp"
      },
      "source": [
        "## Model Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npX8gMU3Y0I"
      },
      "source": [
        "### Weak Learner: One Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dFNBypLJ1nJY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def l2_loss(predictions,Y):\n",
        "    '''\n",
        "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
        "        :param Y: A 1D Numpy array with real values (float64)\n",
        "        :param predictions: A 1D Numpy array of the same size of Y\n",
        "        :return: L2 loss using predictions for Y.\n",
        "    '''\n",
        "\n",
        "    return np.sum((predictions - Y)**2)\n",
        "\n",
        "class OneLayerNN:\n",
        "    '''\n",
        "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        @attrs:\n",
        "            weights: The weights of the neural network model.\n",
        "            batch_size: The number of examples in each batch\n",
        "            learning_rate: The learning rate to use for SGD\n",
        "            epochs: The number of times to pass through the dataset\n",
        "            v: The resulting predictions computed during the forward pass\n",
        "        '''\n",
        "        # initialize self.weights in train()\n",
        "        self.weights = None\n",
        "        self.learning_rate = 0.001\n",
        "        self.epochs = 25\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # initialize self.v in forward_pass()\n",
        "        self.v = None\n",
        "\n",
        "    def train(self, X, Y, print_loss=False):\n",
        "        '''\n",
        "        Trains the OneLayerNN model using SGD.\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :param print_loss: If True, print the loss after each epoch.\n",
        "        :return: None\n",
        "        '''\n",
        "        # Initialize weights\n",
        "        input_size = X.shape[1]\n",
        "        self.weights = np.random.uniform(0,1,(1, input_size))\n",
        "        #print(\"Weights Initial Shape: \", self.weights.shape)\n",
        "\n",
        "        # Train network for certain number of epochs\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # Shuffle the examples (X) and labels (Y)\n",
        "            rand_index = np.arange(X.shape[0])\n",
        "            np.random.shuffle(rand_index)\n",
        "            X_s = X[rand_index]\n",
        "            Y_s = Y[rand_index]\n",
        "\n",
        "             # iterate through the examples in batch size increments\n",
        "            for i in range((int(np.ceil(X_s.shape[0] / self.batch_size)))):\n",
        "                X_batch = X_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "                Y_batch = Y_s[i * self.batch_size : (i + 1) * self.batch_size]\n",
        "\n",
        "                #Perform the forward and backward pass on the current batch\n",
        "                self.forward_pass(X_batch)\n",
        "                self.backward_pass(X_batch, Y_batch)\n",
        "\n",
        "            # Print the loss after every epoch\n",
        "            if print_loss:\n",
        "                print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        '''\n",
        "        Computes the predictions for a single layer given examples X and\n",
        "        stores them in self.v\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :return: None\n",
        "        '''\n",
        "\n",
        "        self.v = np.dot(self.weights, X.T) # + bias (no bias for now?)\n",
        "        #print(\"v shape: \", self.v.shape)\n",
        "\n",
        "\n",
        "\n",
        "    def backward_pass(self, X, Y):\n",
        "        '''\n",
        "        Computes the weights gradient and updates self.weights\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: None\n",
        "        '''\n",
        "        # Compute the gradients for the model's weights using backprop\n",
        "        grads = self.backprop(X,Y)\n",
        "\n",
        "        # Update the weights using gradient descent\n",
        "        self.gradient_descent(grads)\n",
        "\n",
        "\n",
        "\n",
        "    def backprop(self, X, Y):\n",
        "        '''\n",
        "        Returns the average weights gradient for the given batch\n",
        "        :param X: 2D Numpy array where each row contains an example.\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A 1D Numpy array representing the weights gradient\n",
        "        '''\n",
        "        # Compute the average weights gradient\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "\n",
        "        # The gradient dL/dw = -2 * xi * predictons (sum(yi-h(xi)))\n",
        "        m = X.shape[0]\n",
        "        grad_W = (2/m) * np.dot(X.T, self.v - Y)\n",
        "\n",
        "        #print(\"Shape of grad_W: \", grad_W.T.shape)\n",
        "\n",
        "        return grad_W.T\n",
        "\n",
        "    def gradient_descent(self, grad_W):\n",
        "        '''\n",
        "        Updates the weights using the given gradient\n",
        "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
        "        :return: None\n",
        "        '''\n",
        "        # Update the weights using the given gradient and the learning rate\n",
        "        # Refer to the SGD algorithm in slide 12 in Lecture 17: Backpropagation\n",
        "        self.weights = self.weights - (self.learning_rate * grad_W)\n",
        "        #print(self.weights)\n",
        "\n",
        "\n",
        "    def loss(self, X, Y):\n",
        "        '''\n",
        "        Returns the total squared error on some dataset (X, Y).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the squared error of the model on the dataset\n",
        "        '''\n",
        "        # Perform the forward pass and compute the l2 loss\n",
        "        self.forward_pass(X)\n",
        "        return l2_loss(self.v, Y)\n",
        "\n",
        "    def average_loss(self, X, Y):\n",
        "        '''\n",
        "        Returns the mean squared error on some dataset (X, Y).\n",
        "        MSE = Total squared error/# of examples\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "        :return: A float which is the mean squared error of the model on the dataset\n",
        "        '''\n",
        "        return self.loss(X, Y) / X.shape[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Returns the predicted values for some dataset (X).\n",
        "        :param X: 2D Numpy array where each row contains an example\n",
        "        :return: 1D Numpy array containing the predicted values for each example\n",
        "        '''\n",
        "        self.forward_pass(X)\n",
        "        return self.v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3JqPeK3hVC"
      },
      "source": [
        "### Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eDK23Q7F3gZJ"
      },
      "outputs": [],
      "source": [
        "class Boosted_NN:\n",
        "  def __init__(self, n_estimators=50, learning_rate=0.01, random_state=1):\n",
        "    self.n_estimators = n_estimators\n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "    self.estimator_weights = np.zeros(self.n_estimators)\n",
        "\n",
        "    # Initialize the estimators\n",
        "    self.estimators = []\n",
        "    for i in range(self.n_estimators):\n",
        "      self.estimators.append(OneLayerNN())\n",
        "\n",
        "  def train(self, X, y):\n",
        "    '''\n",
        "    Trains/Fits the Boosting Model using AdaBoost.\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :param Y: 1D Numpy array containing the corresponding values for each example\n",
        "    '''\n",
        "    # Initialize the weights\n",
        "    num_inputs = X.shape[0]\n",
        "    self.estimator_weights = 1/num_inputs * np.ones(num_inputs)\n",
        "\n",
        "    # For each round/weak learner\n",
        "    for i in range(self.n_estimators):\n",
        "\n",
        "      # Use the weak learner\n",
        "      weak_learner = self.estimators[i]\n",
        "\n",
        "      # Fit the weak learner\n",
        "      weak_learner.train(X, y)\n",
        "\n",
        "      y_pred = weak_learner.predict(X).reshape(num_inputs)\n",
        "\n",
        "      loss = l2_loss(y, y_pred)\n",
        "      e_t = (self.estimator_weights[i] * loss) / loss\n",
        "      print(\"weighted error\", e_t)\n",
        "      w_t = 0.5 * np.log((1 / e_t) - 1)\n",
        "      print(\"w_t\", w_t)\n",
        "      \n",
        "  \n",
        "      self.estimator_weights *= np.exp(-w_t * y * y_pred)\n",
        "      self.estimator_weights /= np.sum(self.estimator_weights)\n",
        "      print(\"sum of weights (should be 1)\", np.sum(self.estimator_weights))\n",
        "\n",
        "\n",
        "  def loss(self, X, Y):\n",
        "    print(\"X: \", X)\n",
        "    print(\"Y: \", Y)\n",
        "    print(\"first estimator loss: \", self.estimators[0].loss(X,Y))\n",
        "    print(\"second estimator loss: \", self.estimators[1].loss(X,Y))\n",
        "    estimator_loss = np.array([e.loss(X, Y) for e in self.estimators])\n",
        "    print(estimator_loss)\n",
        "    return np.dot(estimator_loss, self.estimator_weights)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    '''\n",
        "    Returns the predicted values for some dataset (X).\n",
        "    :param X: 2D Numpy array where each row contains an example\n",
        "    :return: 1D Numpy array containing the predicted values for each example\n",
        "    '''\n",
        "    y_pred = np.array([e.predict(X) for e in self.estimators])\n",
        "    return np.dot(self.estimator_weights, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjphpjYOI1F"
      },
      "source": [
        "## Accuracy on Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-mPAx8l8ONaB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running models on wine.txt dataset\n",
            "----- 1-Layer NN -----\n",
            "Average Training Loss: 0.5638270145889568\n",
            "Average Testing Loss: 0.5734783902819962\n",
            "----- Boosted Neural Network -----\n",
            "weighted error 0.00025523226135783564\n",
            "w_t 4.136540666832915\n",
            "sum of weights (should be 1) 0.9999999999999999\n",
            "weighted error 2.9764082517316005e-40\n",
            "w_t 45.50634321541903\n",
            "sum of weights (should be 1) 1.0\n",
            "weighted error 0.0\n",
            "w_t inf\n",
            "sum of weights (should be 1) nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ks/kylnnfwn2214tmv9rl7gyb480000gn/T/ipykernel_93356/3829155023.py:37: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  w_t = 0.5 * np.log((1 / e_t) - 1)\n",
            "/var/folders/ks/kylnnfwn2214tmv9rl7gyb480000gn/T/ipykernel_93356/3829155023.py:42: RuntimeWarning: invalid value encountered in divide\n",
            "  self.estimator_weights /= np.sum(self.estimator_weights)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n",
            "weighted error nan\n",
            "w_t nan\n",
            "sum of weights (should be 1) nan\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Boosted_NN' object has no attribute 'average_loss'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Training Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39maverage_loss(X_train_b, Y_train))\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Testing Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39maverage_loss(X_test_b, Y_test))\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtest_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwine.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[45], line 45\u001b[0m, in \u001b[0;36mtest_models\u001b[0;34m(dataset, test_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m Boosted_NN()\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(X_train_b, Y_train)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Training Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage_loss\u001b[49m(X_train_b, Y_train))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Testing Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39maverage_loss(X_test_b, Y_test))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Boosted_NN' object has no attribute 'average_loss'"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "\n",
        "\n",
        "def test_models(dataset, test_size=0.2):\n",
        "    '''\n",
        "        Tests OneLayerNN, Boost on a given dataset.\n",
        "        :param dataset The path to the dataset\n",
        "        :return None\n",
        "    '''\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(dataset):\n",
        "        print('The file {} does not exist'.format(dataset))\n",
        "        exit()\n",
        "\n",
        "    # Load in the dataset\n",
        "    data = np.loadtxt(dataset, skiprows = 1)\n",
        "    X, Y = data[:, 1:], data[:, 0]\n",
        "\n",
        "    # Normalize the features\n",
        "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "\n",
        "    print('Running models on {} dataset'.format(dataset))\n",
        "\n",
        "    # Add a bias\n",
        "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
        "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
        "\n",
        "    #### 1-Layer NN ######\n",
        "    print('----- 1-Layer NN -----')\n",
        "    nnmodel = OneLayerNN()\n",
        "    nnmodel.train(X_train_b, Y_train, print_loss=False)\n",
        "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train))\n",
        "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test))\n",
        "\n",
        "    #### 2-Layer NN ######\n",
        "    print('----- Boosted Neural Network -----')\n",
        "    model = Boosted_NN()\n",
        "\n",
        "    model.train(X_train_b, Y_train)\n",
        "\n",
        "    print('Average Training Loss:', model.average_loss(X_train_b, Y_train))\n",
        "    print('Average Testing Loss:', model.average_loss(X_test_b, Y_test))\n",
        "\n",
        "test_models('wine.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h3Us2FgOMgY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y7jqYzqTDwBc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data2060",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
